{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d65e2f1-a92d-4d43-a351-882d071e8334",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning Project 2023\n",
    "- Beckham Wee 1006010\n",
    "- Shao Ren 1005935\n",
    "- Zheng Wei 1006014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d838c1d",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7590be0",
   "metadata": {},
   "source": [
    "Write a function that estimates the emission parameters from the training set using MLE (maximum\n",
    "likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b373d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        file_content = file.read()\n",
    "    return file_content\n",
    "\n",
    "def write_file(file_path, content):\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5331aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_parameters(file_content):\n",
    "    tags = {}  # Dictionary to store counts of each observation\n",
    "    emission_params = {}  # Dictionary to store emission probabilities\n",
    "\n",
    "    # Split the file content into lines\n",
    "    lines = file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if line[-1] == 'O':\n",
    "            observation, tag = line[:-1], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-10], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-9], line[-9:]\n",
    "#         observation, tag = line.split()\n",
    "    \n",
    "        if tag in tags:\n",
    "            tags[tag][observation] = tags[tag].get(observation, 0) + 1\n",
    "        else:\n",
    "            tags[tag] = {observation: 1}\n",
    "\n",
    "    # Calculate emission probabilities for each observation and tag\n",
    "    for tag, observations_count in tags.items():\n",
    "        total_count = sum(observations_count.values())\n",
    "        emission_params[tag] = {\n",
    "            observation: count / total_count for observation, count in observations_count.items()\n",
    "        }\n",
    "\n",
    "    return emission_params\n",
    "\n",
    "file_content = read_file(\"ES/train.txt\")\n",
    "\n",
    "emission_parameters = estimate_emission_parameters(file_content)\n",
    "# print(emission_parameters, sum(len(words) for words in emission_parameters.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30067806",
   "metadata": {},
   "source": [
    "One problem with estimating the emission parameters is that some words that appear in the test set\n",
    "do not appear in the training set. One simple idea to handle this issue is as follows. We introduce\n",
    "a special word token #UNK#, and make the following modifications to the computation of emission\n",
    "probabilities.During the testing phase, if the word does not appear in the training set, we replace that word with\n",
    "#UNK#. Set k to 1, implement this fix into your function for computing the emission parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b13646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_parameters_modified(training_file_content, test_file_content, k=1):\n",
    "    tags = {}  # Dictionary to store counts of each observation\n",
    "    emission_params = {}  # Dictionary to store emission probabilities\n",
    "    train_words = []\n",
    "    test_words = []\n",
    "    \n",
    "    # Split the file content into lines\n",
    "    train_data_lines = training_file_content.strip().split(\"\\n\")\n",
    "    test_data_lines = test_file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line in train_data_lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if ' O' in line:\n",
    "            observation, tag = line[:-2], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-11], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-10], line[-9:]\n",
    "        if observation not in train_words:\n",
    "            train_words.append(observation)\n",
    "        if tag in tags:\n",
    "            tags[tag][observation] = tags[tag].get(observation, 0) + 1\n",
    "        else:\n",
    "            tags[tag] = {observation: 1}\n",
    "    # Iterate through each line to extract observations from test set\n",
    "    for line in test_data_lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if ' O' in line:\n",
    "            observation, tag = line[:-2], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-11], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-10], line[-9:]\n",
    "        else:\n",
    "            observation = line\n",
    "        if observation not in test_words:\n",
    "            test_words.append(observation)\n",
    "    # Extract words that are in test set but not in train set   \n",
    "    unique_words = find_unique_words(test_words, train_words)\n",
    "\n",
    "    # Calculate emission probabilities for each observation and tag\n",
    "    for tag, observations_count in tags.items():\n",
    "        total_count = sum(observations_count.values())\n",
    "        emission_params[tag] = {\n",
    "            observation: count / (total_count + k) for observation, count in observations_count.items()\n",
    "        }\n",
    "        for word in unique_words:\n",
    "            emission_params[tag][word] = k / (total_count + k)\n",
    "\n",
    "\n",
    "    return emission_params, list(set(test_words).union(train_words))\n",
    "\n",
    "def find_unique_words(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    unique_in_list1 = set1 - set2\n",
    "\n",
    "    return list(unique_in_list1)\n",
    "\n",
    "\n",
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "emission_parameters = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "# print(emission_parameters, sum(len(words) for words in emission_parameters.values()))\n",
    "count = 0\n",
    "for emission_probabilities in emission_parameters.values():\n",
    "    count += len(emission_probabilities)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c12afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(path_to_train_file, path_to_test_file):\n",
    "    training_file_content = read_file(path_to_train_file)\n",
    "    test_file_content = read_file(path_to_test_file)\n",
    "    emission_parameters, words = estimate_emission_parameters_modified(training_file_content, test_file_content)\n",
    "    word_to_label = {}\n",
    "    for word in words:\n",
    "        probabilities = []\n",
    "        for label, freqs in emission_parameters.items():\n",
    "            if word in freqs:\n",
    "                probabilities.append((label, freqs[word]))\n",
    "        word_to_label[word] = max(probabilities, key=lambda x: x[1])[0]\n",
    "    return word_to_label\n",
    "\n",
    "def write_result_to_file(word_to_label, path_to_dev_set, path_to_output):\n",
    "    result = \"\"\n",
    "    dev_file_content = read_file(path_to_dev_set)\n",
    "    dev_set_lines = dev_file_content.strip().split(\"\\n\")\n",
    "    counter = 0\n",
    "    for line in dev_set_lines:\n",
    "        if line == \"\":\n",
    "            result += \"\\n\"\n",
    "            continue  \n",
    "        result += line + \" \" + word_to_label.get(line) + \"\\n\"\n",
    " \n",
    "    write_file(path_to_output, result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b9f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = sentiment_analysis(\"ES/train.txt\", \"ES/dev.in\")\n",
    "write_result_to_file(mapping, \"ES/dev.in\", \"ES/dev.p1.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03317db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = sentiment_analysis(\"RU/train.txt\", \"RU/dev.in\")\n",
    "write_result_to_file(mapping, \"RU/dev.in\", \"RU/dev.p1.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18d25a",
   "metadata": {},
   "source": [
    "## ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 1466\n",
    "\n",
    "#Correct Entity : 178\n",
    "Entity  precision: 0.1214\n",
    "Entity  recall: 0.7773\n",
    "Entity  F: 0.2100\n",
    "\n",
    "#Correct Sentiment : 97\n",
    "Sentiment  precision: 0.0662\n",
    "Sentiment  recall: 0.4236\n",
    "Sentiment  F: 0.1145\n",
    "\n",
    "## RU\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 1816\n",
    "\n",
    "#Correct Entity : 266\n",
    "Entity  precision: 0.1465\n",
    "Entity  recall: 0.6838\n",
    "Entity  F: 0.2413\n",
    "\n",
    "#Correct Sentiment : 129\n",
    "Sentiment  precision: 0.0710\n",
    "Sentiment  recall: 0.3316\n",
    "Sentiment  F: 0.1170"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64518f2",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cb2b5",
   "metadata": {},
   "source": [
    "Write a function that estimates the transition parameters from the training set using MLE (maximum\n",
    "likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62408ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'START': {'O': 0.9289176090468497, 'B-positive': 0.052234787291330104, 'B-negative': 0.014001077005923533, 'B-neutral': 0.004846526655896607}, 'O': {'O': 0.8856896848630963, 'B-positive': 0.03650766316514551, 'STOP': 0.06344067504735663, 'B-negative': 0.012226623041157224, 'B-neutral': 0.0021353538832443605}, 'B-positive': {'O': 0.871551724137931, 'I-positive': 0.11637931034482758, 'STOP': 0.008620689655172414, 'B-neutral': 0.0008620689655172414, 'B-positive': 0.002586206896551724}, 'B-negative': {'O': 0.8110236220472441, 'I-negative': 0.1784776902887139, 'STOP': 0.010498687664041995}, 'B-neutral': {'I-neutral': 0.20833333333333334, 'O': 0.7916666666666666}, 'I-neutral': {'I-neutral': 0.6511627906976745, 'O': 0.3488372093023256}, 'I-positive': {'I-positive': 0.5700636942675159, 'O': 0.4267515923566879, 'STOP': 0.0031847133757961785}, 'I-negative': {'O': 0.39766081871345027, 'I-negative': 0.6023391812865497}}\n"
     ]
    }
   ],
   "source": [
    "def estimate_transition_parameters(file_content):\n",
    "    transition_counts = {}\n",
    "    transition_params = {}  # Dictionary to store emission probabilities\n",
    "\n",
    "    # Split the file content into lines\n",
    "    lines = file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line_index in range(len(lines)):\n",
    "        next_line = \"\"\n",
    "        curr_line = lines[line_index]\n",
    "        if line_index < len(lines) - 1:\n",
    "            next_line = lines[line_index + 1]\n",
    "        \n",
    "        if len(curr_line) == 0:\n",
    "            continue\n",
    "        if curr_line[-1] == 'O':\n",
    "            curr_observation, curr_tag = curr_line[:-1], curr_line[-1]\n",
    "        elif ('B-positive' in curr_line \n",
    "            or 'I-positive' in curr_line \n",
    "            or 'B-negative' in curr_line \n",
    "            or 'I-negative' in curr_line):\n",
    "            curr_observation, curr_tag = curr_line[:-10], curr_line[-10:]\n",
    "        elif ('B-neutral' in curr_line\n",
    "            or 'I-neutral' in curr_line):\n",
    "            curr_observation, curr_tag = curr_line[:-9], curr_line[-9:]\n",
    "        \n",
    "        if len(next_line) == 0: #true if line_index = len(lines - 1) or line_index is last index of a document\n",
    "            pass\n",
    "        elif next_line[-1] == 'O':\n",
    "            next_observation, next_tag = next_line[:-1], next_line[-1]\n",
    "        elif ('B-positive' in next_line \n",
    "            or 'I-positive' in next_line \n",
    "            or 'B-negative' in next_line \n",
    "            or 'I-negative' in next_line):\n",
    "            next_observation, next_tag = next_line[:-10], next_line[-10:]\n",
    "        elif ('B-neutral' in next_line\n",
    "            or 'I-neutral' in next_line):\n",
    "            next_observation, next_tag = next_line[:-9], next_line[-9:]        \n",
    "        \n",
    "        # Handle transitions from START\n",
    "        if line_index == 0 or lines[line_index - 1] == \"\":\n",
    "            if 'START' in transition_counts:\n",
    "                transition_counts['START'][curr_tag] = transition_counts['START'].get(curr_tag, 0) + 1\n",
    "            else:\n",
    "                transition_counts['START'] = {curr_tag: 1}\n",
    "                \n",
    "        # Handle transitions to STOP\n",
    "        if len(next_line) == 0:\n",
    "            if curr_tag in transition_counts:\n",
    "                transition_counts[curr_tag]['STOP'] = transition_counts[curr_tag].get('STOP', 0) + 1\n",
    "            else:\n",
    "                transition_counts[curr_tag] = {'STOP': 1}\n",
    "                \n",
    "        # Rest of the transitions\n",
    "        else:\n",
    "            if curr_tag in transition_counts:\n",
    "                transition_counts[curr_tag][next_tag] = transition_counts[curr_tag].get(next_tag, 0) + 1\n",
    "            else:\n",
    "                transition_counts[curr_tag] = {next_tag: 1}\n",
    "        \n",
    "\n",
    "    # Calculate transition probabilities \n",
    "    for tag, next_tag_counts in transition_counts.items():\n",
    "        total_count = sum(next_tag_counts.values())\n",
    "        transition_params[tag] = {\n",
    "            next_tag: count / total_count for next_tag, count in next_tag_counts.items()\n",
    "        }\n",
    "\n",
    "    return transition_params\n",
    "\n",
    "file_content = read_file(\"ES/train.txt\")\n",
    "\n",
    "transition_parameters = estimate_transition_parameters(file_content)\n",
    "print(transition_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181cb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(observation_sequence, emission_params, transition_params):\n",
    "    states = list(emission_params.keys())\n",
    "    T = len(observation_sequence)\n",
    "    N = len(states)\n",
    "\n",
    "    # Initialization step\n",
    "    viterbi_table = [{}]\n",
    "    backpointer = [{}]\n",
    "    for state in states:\n",
    "        viterbi_table[0][state] = transition_params['START'].get(state, 0) * emission_params[state].get(observation_sequence[0], 0)\n",
    "        backpointer[0][state] = None\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, T):\n",
    "        viterbi_table.append({})\n",
    "        backpointer.append({})\n",
    "        for state in states:\n",
    "            max_prob = max(\n",
    "                viterbi_table[t - 1][prev_state] * transition_params[prev_state].get(state, 0) * emission_params[state].get(observation_sequence[t], 0)\n",
    "                for prev_state in states\n",
    "            )\n",
    "            viterbi_table[t][state] = max_prob\n",
    "            backpointer[t][state] = max(states, key=lambda prev_state: viterbi_table[t - 1][prev_state] * transition_params[prev_state].get(state, 0))\n",
    "\n",
    "    # Termination step\n",
    "    max_prob_last_state = max(viterbi_table[T - 1].values())\n",
    "    best_last_state = max(states, key=lambda state: viterbi_table[T - 1][state] * transition_params[state].get('STOP', 0))\n",
    "\n",
    "    # Backtrack to find the best sequence\n",
    "    best_sequence = [best_last_state]\n",
    "    prev_state = best_last_state\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        best_sequence.insert(0, backpointer[t + 1][prev_state])\n",
    "        prev_state = backpointer[t + 1][prev_state]\n",
    "\n",
    "    return best_sequence\n",
    "\n",
    "def create_observation_sequences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        file_content = file.read().strip()\n",
    "\n",
    "    observation_sequences = []\n",
    "    current_sequence = []\n",
    "\n",
    "    # Split the file content by empty lines to get individual sequences\n",
    "    sequence_lines = file_content.split(\"\\n\\n\")\n",
    "\n",
    "    for sequence_line in sequence_lines:\n",
    "        # Split each sequence into individual observations (words)\n",
    "        observation_sequence = sequence_line.strip().split(\"\\n\")\n",
    "        current_sequence.extend(observation_sequence)\n",
    "        observation_sequences.append(current_sequence)\n",
    "        current_sequence = []\n",
    "\n",
    "    return observation_sequences\n",
    "\n",
    "def viterbi_for_sequences(observation_sequences, emission_params, transition_params):\n",
    "    best_sequences = []\n",
    "    for observation_sequence in observation_sequences:\n",
    "        best_sequence = viterbi(observation_sequence, emission_params, transition_params)\n",
    "        best_sequences.append(best_sequence)\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "def write_sequences_to_file(output_file_path, observation_sequences, best_sequences):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for obs_sequence, best_sequence in zip(observation_sequences, best_sequences):\n",
    "            for word, tag in zip(obs_sequence, best_sequence):\n",
    "                file.write(f\"{word} {tag}\\n\")\n",
    "            file.write(\"\\n\")  # Separate sequences with an empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a315bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "emission_params = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "transition_params = estimate_transition_parameters(training_file_content)\n",
    "observation_sequences = create_observation_sequences(\"ES/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"ES/dev.p2.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dac0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_content = read_file(\"RU/train.txt\")\n",
    "test_file_content = read_file(\"RU/dev.in\")\n",
    "emission_params = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "transition_params = estimate_transition_parameters(training_file_content)\n",
    "observation_sequences = create_observation_sequences(\"RU/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"RU/dev.p2.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56d1b4",
   "metadata": {},
   "source": [
    "## ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 542\n",
    "\n",
    "#Correct Entity : 134\n",
    "Entity  precision: 0.2472\n",
    "Entity  recall: 0.5852\n",
    "Entity  F: 0.3476\n",
    "\n",
    "#Correct Sentiment : 97\n",
    "Sentiment  precision: 0.1790\n",
    "Sentiment  recall: 0.4236\n",
    "Sentiment  F: 0.2516"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d21ca7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RU\n",
    "\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 514\n",
    "\n",
    "#Correct Entity : 190\n",
    "Entity  precision: 0.3696\n",
    "Entity  recall: 0.4884\n",
    "Entity  F: 0.4208\n",
    "\n",
    "#Correct Sentiment : 129\n",
    "Sentiment  precision: 0.2510\n",
    "Sentiment  recall: 0.3316\n",
    "Sentiment  F: 0.2857"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be824b",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "Use the estimated transition and emission parameters, implement an algorithm to find the k-th best\n",
    "output sequences (if there are multiple sequences that are k-th best, output any of them). Clearly\n",
    "describe the steps of your algorithm in your report.\n",
    "\n",
    "Run the algorithm on the development sets RU/dev.in and ES/dev.in to generate 2-nd best\n",
    "and 8-th best outputs. Write the outputs to RU/dev.p3.2nd.out, RU/dev.p3.8th.out and\n",
    "ES/dev.p3.2nd.out, ES/dev.p3.8th.out. Report the precision, recall and F scores for the\n",
    "outputs for both languages.\n",
    "\n",
    "Hint: find the top-k best sequences using dynamic programming by modifying the original Viterbi\n",
    "algorithm.\n",
    "\n",
    "## Approach\n",
    "### Revamped Viterbi Implementation:\n",
    "The Viterbi algorithm is extended to maintain a list of candidate sequences for each state at each step.\n",
    "- This list is sorted based on probabilities, and the top-k sequences are kept.\n",
    "- The algorithm explores multiple sequences in parallel, allowing for the possibility of obtaining the k-th best sequence.\n",
    "\n",
    "The revamped Viterbi implementation's modification allows it to generate and evaluate multiple sequence candidates, making it suitable for obtaining the k-th best sequence. This is why the revamped approach can achieve the desired k-th best output while the first implementation is designed for finding the single best output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c71dc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# get 2 lists, one for observations one for labels from training data\n",
    "def read_observations_labels(input_file):\n",
    "    # Initialize lists to store processed data\n",
    "    all_observations = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Open the input file for reading\n",
    "    with open(input_file, 'r') as f:\n",
    "        # Initialize temporary lists for the current group of observations and labels\n",
    "        current_observation_group = []\n",
    "        current_label_group = []\n",
    "        \n",
    "        # Iterate over each line in the file\n",
    "        for line in f:\n",
    "            # Check if the line is empty or contains only whitespace\n",
    "            if line.isspace():\n",
    "                # End of group: Append current observations and labels to the main lists\n",
    "                all_observations.append(current_observation_group[:])\n",
    "                all_labels.append(current_label_group[:])\n",
    "                \n",
    "                # Clear temporary lists to prepare for the next group\n",
    "                current_observation_group.clear()\n",
    "                current_label_group.clear()\n",
    "            else:\n",
    "                # Extract observation and label from the line\n",
    "                observation, label = line.strip().rsplit(maxsplit=1)\n",
    "                \n",
    "                # Add the extracted observation and label to their respective lists\n",
    "                current_observation_group.append(observation)\n",
    "                current_label_group.append(label)\n",
    "    \n",
    "    # Return the lists of processed observations and labels\n",
    "    return all_observations, all_labels\n",
    "\n",
    "\n",
    "def revamped_transition_estimator(data):\n",
    "    # Initialize a dictionary of dictionaries to store transition parameters\n",
    "    trans_params = defaultdict(lambda: defaultdict(int))\n",
    "    # Initialize count for the 'START' and 'STOP' transitions\n",
    "    trans_params['START']['COUNT'] = 0\n",
    "    trans_params['STOP']['COUNT'] = 0\n",
    "    \n",
    "    # Iterate through each sequence of labels in the input data\n",
    "    for seq in data:\n",
    "        prev_label = 'START'  # Initialize previous label as 'START'\n",
    "        for curr_label in seq:\n",
    "            # Count the transition from prev_label to curr_label\n",
    "            trans_params[prev_label][curr_label] += 1\n",
    "            # Increment the total count for transitions from prev_label\n",
    "            trans_params[prev_label]['COUNT'] += 1\n",
    "            prev_label = curr_label  # Update prev_label to current label\n",
    "        \n",
    "        # Count the transition to 'STOP' at the end of the sequence\n",
    "        trans_params[prev_label]['STOP'] += 1\n",
    "        # Increment the total count for transitions from prev_label\n",
    "        trans_params[prev_label]['COUNT'] += 1\n",
    "    \n",
    "    # Calculate log-probabilities for the transitions\n",
    "    log_probs = {}\n",
    "    for prev_label, trans_counts in trans_params.items():\n",
    "        log_probs[prev_label] = {}\n",
    "        total_trans = trans_counts['COUNT']  # Total count of transitions from prev_label\n",
    "        for curr_label, count in trans_counts.items():\n",
    "            if curr_label != 'COUNT':  # Skip the 'COUNT' key\n",
    "                # Calculate the log-probability for the transition\n",
    "                log_probs[prev_label][curr_label] = np.log(count / total_trans)\n",
    "    \n",
    "    return log_probs\n",
    "\n",
    "\n",
    "def revamped_emission_parameters(observations, labels, k=1):\n",
    "    # Dictionary to store emission parameters for each label\n",
    "    emission_params = {}\n",
    "    # Dictionary to store vocabulary words\n",
    "    transformed_vocab = {}\n",
    "    \n",
    "    # Loop through each sequence of labels and corresponding observations\n",
    "    for lbl_idx in range(len(labels)):\n",
    "        for obs_idx in range(len(labels[lbl_idx])):\n",
    "            # Get the current label and observation\n",
    "            current_label = labels[lbl_idx][obs_idx]\n",
    "            current_observation = observations[lbl_idx][obs_idx]\n",
    "            \n",
    "            # If the current label is not in emission_params, initialize it\n",
    "            if current_label not in emission_params:\n",
    "                emission_params[current_label] = {'COUNT': k, '#UNK#': k}\n",
    "            \n",
    "            # Get the current count of the observation for the current label\n",
    "            curr_count = emission_params[current_label].get(current_observation, 0)\n",
    "            # Increment the count of the current observation for the current label\n",
    "            emission_params[current_label][current_observation] = curr_count + 1\n",
    "            # Increment the total count for the current label\n",
    "            emission_params[current_label]['COUNT'] += 1\n",
    "            # Add the current observation to the vocabulary\n",
    "            transformed_vocab[current_observation] = True\n",
    "\n",
    "    # Calculate log-probabilities for each emission parameter\n",
    "    for lbl in emission_params.keys():\n",
    "        for obs in emission_params[lbl].keys():\n",
    "            if obs != 'COUNT':\n",
    "                # Calculate and store the log-probability of the emission parameter\n",
    "                emission_params[lbl][obs] = np.log(emission_params[lbl][obs] / emission_params[lbl]['COUNT'])\n",
    "                \n",
    "    return emission_params, transformed_vocab\n",
    "\n",
    "\n",
    "def kth_best_viterbi(word_dict, sentence, trans_params, emiss_params, k_val):\n",
    "    # Prepare the list of labels excluding 'START' and 'STOP'\n",
    "    label_list = list(trans_params.keys())\n",
    "    label_list.remove('START')\n",
    "    label_list.remove('STOP')\n",
    "    word_count = len(sentence)\n",
    "    \n",
    "    # Initialize the viterbi_table with the 'START' label and its score\n",
    "    viterbi_table = []\n",
    "    viterbi_table.append({'START': [(0, [])]})\n",
    "\n",
    "    # Loop through each word in the sentence\n",
    "    for i in range(word_count):\n",
    "        viterbi_table.append({})  # Create a new row for the current word\n",
    "        if word_dict.get(sentence[i], False):\n",
    "            current_obs = sentence[i]  # Use the word itself if it's in the dictionary\n",
    "        else:\n",
    "            current_obs = '#UNK#'  # Use '#UNK#' if the word is not in the dictionary\n",
    "\n",
    "        # Loop through each possible next label\n",
    "        for next_label in label_list:\n",
    "            obs_score = emiss_params[next_label].get(current_obs, -float('inf'))  # Get emission score\n",
    "\n",
    "            entries = []\n",
    "            # Loop through each possible previous label\n",
    "            for prev_label in trans_params.keys():\n",
    "                prev_entries = viterbi_table[i].get(prev_label, [])  # Get previous entries in the table\n",
    "                for v_score, path in prev_entries:\n",
    "                    trans_score = trans_params.get(prev_label, {}).get(next_label, -float('inf'))  # Get transition score\n",
    "                    new_score = v_score + obs_score + trans_score  # Calculate combined score\n",
    "                    if new_score > -float('inf'):\n",
    "                        new_path = path.copy()\n",
    "                        new_path.append(prev_label)\n",
    "                        entries.append((new_score, new_path))\n",
    "            entries.sort(key=lambda x: x[0])  # Sort entries based on score\n",
    "            while len(entries) > k_val:\n",
    "                entries.pop(0)  # Keep only top-k entries\n",
    "            if len(entries) > 0:\n",
    "                viterbi_table[i + 1][next_label] = entries  # Store the top-k entries in the table\n",
    "\n",
    "        # Handle the case where no label is assigned to the current word\n",
    "        if len(list(viterbi_table[i + 1].keys())) < 1:\n",
    "            entries = []\n",
    "            for prev_label in trans_params.keys():\n",
    "                prev_entries = viterbi_table[i].get(prev_label, [])\n",
    "                for v_score, path in prev_entries:\n",
    "                    new_path = path.copy()\n",
    "                    new_path.append(prev_label)\n",
    "                    entries.append((v_score, new_path))\n",
    "            entries.sort(key=lambda x: x[0])\n",
    "            while len(entries) > k_val:\n",
    "                entries.pop(0)\n",
    "            if len(entries) > 0:\n",
    "                viterbi_table[i + 1]['O'] = entries\n",
    "\n",
    "    # Calculate the entries for the 'STOP' label\n",
    "    entries = []\n",
    "    for prev_label in trans_params.keys():\n",
    "        prev_entries = viterbi_table[word_count].get(prev_label, [])\n",
    "        for v_score, path in prev_entries:\n",
    "            stop_score = trans_params.get(prev_label, {}).get('STOP', -float('inf'))\n",
    "            new_score = v_score + stop_score\n",
    "            if new_score > -float('inf'):\n",
    "                new_path = path.copy()\n",
    "                new_path.append(prev_label)\n",
    "                entries.append((new_score, new_path))\n",
    "    entries.sort(key=lambda x: x[0])\n",
    "    while len(entries) > k_val:\n",
    "        entries.pop(0)\n",
    "    viterbi_table.append({'STOP': entries})  # Store the entries for 'STOP' label\n",
    "    \n",
    "    # Retrieve the final sequence by selecting the path with the highest probability\n",
    "    final_seq = viterbi_table[-1]['STOP'][0][1]\n",
    "    final_seq.pop(0)  # Remove the initial 'START' label\n",
    "    return final_seq\n",
    "\n",
    "# run prediction and save result to file\n",
    "def write_labels_to_file(emission_parameters, transition_parameters, vocab, k, input_file, output_file):\n",
    "    observations = create_observation_sequences(input_file)\n",
    "    with open(output_file, 'w') as f:\n",
    "        for observation in observations:\n",
    "            labels = kth_best_viterbi(vocab, observation, transition_parameters, emission_parameters, k)\n",
    "            for i in range(len(observation)):\n",
    "                f.write(observation[i] + ' ' + labels[i] + '\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438289d4",
   "metadata": {},
   "source": [
    "## Prediction for ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8091f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, labels = read_observations_labels('ES/train.txt')\n",
    "transition_parameters = revamped_transition_estimator(labels)\n",
    "emission_parameters, vocab = revamped_emission_parameters(observations, labels)\n",
    "\n",
    "#generate output for k = 2 and k = 8\n",
    "k = [2, 8]\n",
    "for _k in k:\n",
    "    if _k == 2:\n",
    "        filename = 'dev.p3.2nd.out'\n",
    "    else:\n",
    "        filename = f'dev.p3.{_k}th.out'\n",
    "    write_labels_to_file(emission_parameters, transition_parameters, vocab, _k, 'ES/dev.in', 'ES/' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cdb14",
   "metadata": {},
   "source": [
    "### Results for ES\n",
    "#### k=2\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 438\n",
    "\n",
    "#Correct Entity : 119\n",
    "Entity  precision: 0.2717\n",
    "Entity  recall: 0.5197\n",
    "Entity  F: 0.3568\n",
    "\n",
    "#Correct Sentiment : 67\n",
    "Sentiment  precision: 0.1530\n",
    "Sentiment  recall: 0.2926\n",
    "Sentiment  F: 0.2009\n",
    "\n",
    "#### k=8\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 485\n",
    "\n",
    "#Correct Entity : 109\n",
    "Entity  precision: 0.2247\n",
    "Entity  recall: 0.4760\n",
    "Entity  F: 0.3053\n",
    "\n",
    "#Correct Sentiment : 60\n",
    "Sentiment  precision: 0.1237\n",
    "Sentiment  recall: 0.2620\n",
    "Sentiment  F: 0.1681"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a1ee2",
   "metadata": {},
   "source": [
    "## Prediction for RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a80497",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, labels = read_observations_labels('RU/train.txt')\n",
    "transition_parameters = revamped_transition_estimator(labels)\n",
    "emission_parameters, vocab = revamped_emission_parameters(observations, labels)\n",
    "\n",
    "#generate output for k = 2 and k = 8\n",
    "k = [2, 8]\n",
    "for _k in k:\n",
    "    if _k == 2:\n",
    "        filename = 'dev.p3.2nd.out'\n",
    "    else:\n",
    "        filename = f'dev.p3.{_k}th.out'\n",
    "    write_labels_to_file(emission_parameters, transition_parameters, vocab, _k, 'RU/dev.in', 'RU/' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b726c61",
   "metadata": {},
   "source": [
    "### Results for RU\n",
    "#### k=2\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 700\n",
    "\n",
    "#Correct Entity : 203\n",
    "Entity  precision: 0.2900\n",
    "Entity  recall: 0.5219\n",
    "Entity  F: 0.3728\n",
    "\n",
    "#Correct Sentiment : 126\n",
    "Sentiment  precision: 0.1800\n",
    "Sentiment  recall: 0.3239\n",
    "Sentiment  F: 0.2314\n",
    "\n",
    "#### k=8\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 801\n",
    "\n",
    "#Correct Entity : 188\n",
    "Entity  precision: 0.2347\n",
    "Entity  recall: 0.4833\n",
    "Entity  F: 0.3160\n",
    "\n",
    "#Correct Sentiment : 102\n",
    "Sentiment  precision: 0.1273\n",
    "Sentiment  recall: 0.2622\n",
    "Sentiment  F: 0.1714"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875a221-d46d-460a-a531-c516476069e6",
   "metadata": {},
   "source": [
    "# Part 4: Design Challenge PART 1\n",
    "For our design, we propose the use of Higher Order Hidden Markov Models (HOHMM). \n",
    "In a Higher Order HMM, emission pand transition probabilities are conditioned on the current state and the previous several states. This allows the model to capture longer-range dependencies in the sequence. We hypothesise that this could help capture more complex sentiment patterns that extend over multiple words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab635ea-c0d8-4528-9f80-26fa06bf33ac",
   "metadata": {},
   "source": [
    "## 4.1 Data Processing\n",
    "Here, we aim to use integers to index each possible state in order. This is done so as to enable the use of numpy arrays for calculating of Higher-order Transition Probabilities, that take into account not just the previous state but one before it that will be done in section `4.2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3583ba2e-fe32-43a6-bdf2-017f63c2345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5382e56-e337-46ec-a674-e304de702546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for this step 4.1\n",
    "def read_data_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Seperates txt file into texts and labels arary\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                text = parts[0]\n",
    "                label = parts[1]\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "            elif len(parts) == 1:\n",
    "                texts.append(line.strip())\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "def process_data_HOHMM(file_path):\n",
    "    \"\"\"\n",
    "    Finds higher order transition probabilities\n",
    "    INPUTS:\n",
    "    - file_path (string): Path to the data file in txt format\n",
    "    OUTPUTS:\n",
    "    - texts: array containing all texts\n",
    "    - labels: array containing coressponding labels of each text\n",
    "    - states: array containing unique states\n",
    "    - state_to_idx: mapping of states to a numerical index\n",
    "    - label_idx: array of states in numerical idx, corresponding to texts\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    texts, labels = read_data_from_file(file_path)\n",
    "    # Get states\n",
    "    states = list(set(labels))\n",
    "    \n",
    "    # Define a mapping from states to indices\n",
    "    state_to_idx = {state: idx for idx, state in enumerate(states)}\n",
    "    \n",
    "    # Get array of state indices corresponding to each text in the texts array\n",
    "    label_idx = [state_to_idx[label] for label in labels]\n",
    "\n",
    "    return texts, labels, states, state_to_idx, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acb8b9f0-e3c2-4d6f-970c-aa80c1462ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing for EU and RS\n",
    "ES_train = \"ES/train.txt\"\n",
    "RU_train = \"RU/train.txt\"\n",
    "\n",
    "ES_texts, ES_labels, ES_states, ES_state_mappings, ES_label_idx = process_data_HOHMM(ES_train)\n",
    "RU_texts, RU_labels, RU_states, RU_state_mappings, RU_label_idx = process_data_HOHMM(RU_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6bbda-fe27-418f-82e9-3556d04cee31",
   "metadata": {},
   "source": [
    "## 4.2 Calculation of Higher Order Transition Probabilities\n",
    "In addition to standard HMM initialization, we consider the higher order transition probabilities. These probabilities involve multiple previous states, capturing longer-range dependencies.\n",
    "\n",
    "Simply put, we need to find the parameter $a_{u, v, q}$ which is the probability of state $q$ appearing after $u, v$ appears.\n",
    "$$a_{u, v, q} = P(q|y, v) = \\frac{Count(u, v, q)}{Count(u,v)}$$\n",
    "\n",
    "whereby $Count(u, v, q)$ is the number of times `q` appears after `u, v` appears before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3131471-b2e0-45db-9065-8fbc9ac54014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_a_HOHMM(states, label_idx):\n",
    "    \"\"\"\n",
    "    Finds a_uvq, HO transition probabilities\n",
    "    INPUTS:\n",
    "    - states (arr): array containing states in dataset\n",
    "    - label_udx (arr): Array of Indexed Labels for each corresponding text\n",
    "    OUTPUTS:\n",
    "    - a_uvq (np.array): np.array containign transition probabilities\n",
    "    \"\"\"\n",
    "    # Find number of states\n",
    "    num_states = len(states)\n",
    "    \n",
    "    # Initiate a np array to store HO a_uvq\n",
    "    transition_probs = np.zeros((num_states, num_states, num_states))\n",
    "    # Finding Count(u, v, q)\n",
    "    for i in range(2, len(label_idx)):\n",
    "        prev_state_1 = label_idx[i - 2]\n",
    "        prev_state_2 = label_idx[i - 1]\n",
    "        current_state = label_idx[i]\n",
    "        \n",
    "        transition_probs[prev_state_1, prev_state_2, current_state] += 1\n",
    "        \n",
    "    # Finding HO transition params\n",
    "    count_uv = np.nansum(transition_probs, axis=2, keepdims=True) + 1e-10\n",
    "    a_uvq = transition_probs / count_uv\n",
    "    \n",
    "    # Handle NaNs, convert to 0 probability\n",
    "    a_uvq = np.nan_to_num(a_uvq, nan=0.0)\n",
    "\n",
    "    return a_uvq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ffd302a-6eb1-4818-9806-8ff8ace87549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding HO transition probabiltiies for ES and RU\n",
    "ES_a = find_a_HOHMM(ES_states, ES_label_idx)\n",
    "RU_a = find_a_HOHMM(RU_states, RU_label_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971673f-c19f-4d01-a443-511aeb8b83bd",
   "metadata": {},
   "source": [
    "## 4.3 Calculating Emission Probabilities\n",
    "Follows that of normal HMM\n",
    "$$b_u(o) = \\frac{Count(u \\to o)}{Count(u)}$$\n",
    "\n",
    "However, we modify the function to handle np arrays. In order to take into account possible unique words, we create a vocab mapping too to prevent IndexErrors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dec43ea-7883-4b45-a39a-c607d9361751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_emission_probs(texts, labels, states):\n",
    "    \"\"\"\n",
    "    Calculates emission probabilities for a Higher Order Hidden Markov Model (HOHMM).\n",
    "\n",
    "    INPUTS:\n",
    "    - texts (list): List of observed words/tokens.\n",
    "    - labels (list): List of corresponding sentiment labels.\n",
    "    - states (list): List of all possible states (sentiment labels).\n",
    "\n",
    "    OUTPUTS:\n",
    "    - emission_probs (np.array): Emission probabilities matrix (state x word).\n",
    "    - words_to_idx (dict): Mappings from word to idx, following emissions probability np array\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    num_words = len(set(texts))\n",
    "    \n",
    "    # Emission probabilities (word given state)\n",
    "    emission_probs = np.zeros((num_states, num_words))\n",
    "    \n",
    "    # Count occurrences of words for each state\n",
    "    state_word_counts = np.zeros((num_states, num_words))\n",
    "\n",
    "    # Mapping dict\n",
    "    words_to_idx = {}\n",
    "    idx_to_words = {}\n",
    "    \n",
    "    # Process the dataset\n",
    "    for i in range(len(texts)):\n",
    "        state_idx = labels[i]  # Use label directly\n",
    "        word = texts[i]\n",
    "        word_idx = hash(word) % num_words  # Simplified hashing\n",
    "\n",
    "        # Store mappings\n",
    "        words_to_idx[word] = word_idx\n",
    "        idx_to_words[word_idx] = word\n",
    "        state_word_counts[state_idx, word_idx] += 1\n",
    "    \n",
    "    # Calculate emission probabilities\n",
    "    for state_idx in range(num_states):\n",
    "        total_count = np.sum(state_word_counts[state_idx])\n",
    "        emission_probs[state_idx] = state_word_counts[state_idx] / total_count\n",
    "    \n",
    "    return emission_probs, words_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beebf78a-247f-4735-a5e4-44e20210d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RU_b, RU_wordmapping = calculate_emission_probs(RU_texts, RU_label_idx, RU_states)\n",
    "ES_b, ES_wordmapping = calculate_emission_probs(ES_texts, ES_label_idx, ES_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c00e1-1f98-44b6-9e3c-f99f644a2b2f",
   "metadata": {},
   "source": [
    "## 4.4 Start Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d49eb47-d751-49e9-8ce5-e629d4169503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_start_probs(label_indices, num_states):\n",
    "    \"\"\"\n",
    "    Calculates start probabilities for a Higher Order Hidden Markov Model (HOHMM).\n",
    "\n",
    "    INPUTS:\n",
    "    - label_indices (list): List of label indices corresponding to each observation.\n",
    "    - num_states (int): Number of possible states (sentiment labels).\n",
    "\n",
    "    OUTPUTS:\n",
    "    - start_probs (np.array): Start probabilities vector for each state.\n",
    "    \"\"\"\n",
    "    start_counts = np.zeros(num_states)\n",
    "    \n",
    "    # Count occurrences of each state as the starting state\n",
    "    start_counts[label_indices[0]] += 1\n",
    "    \n",
    "    for i in range(1, len(label_indices)):\n",
    "        if label_indices[i - 1] != label_indices[i]:\n",
    "            start_counts[label_indices[i]] += 1\n",
    "    \n",
    "    # Calculate start probabilities\n",
    "    total_starts = np.sum(start_counts)\n",
    "    start_probs = start_counts / total_starts\n",
    "    \n",
    "    return start_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64c26d44-28bf-419b-8289-eb6019ec9582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Probabilities for ES: [0.02094241 0.004363   0.11082024 0.33653287 0.01977894 0.46829552\n",
      " 0.03926702]\n",
      "Start Probabilities for RU: [0.0378497  0.00511977 0.08118486 0.33900165 0.0149936  0.45785336\n",
      " 0.06399707]\n"
     ]
    }
   ],
   "source": [
    "ES_start_probs = calculate_start_probs(ES_label_idx, len(ES_states))\n",
    "RU_start_probs = calculate_start_probs(RU_label_idx, len(RU_states))\n",
    "\n",
    "print(\"Start Probabilities for ES:\", ES_start_probs)\n",
    "print(\"Start Probabilities for RU:\", RU_start_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f9e89-3169-4f56-bd98-125f1302d082",
   "metadata": {},
   "source": [
    "## 4.5 Viterbi Algorithm for Best Sequences\n",
    "The function belows is differnt from the first due to the need of handling np.arrays, as well as a different way of calculating forward scores. Normally,\n",
    "$$\\pi(j+1, u) = \\max_v \\{\\pi(j, v) \\times b_u(x_{j+1}) \\times a_{v, u} \\}$$\n",
    "However, we need to take into account that we are not just using the score of the previous states, but that of 2 states ago as well. Hence,\n",
    "$$\\pi(j+1, q) = \\max_{u, v} \\{\\pi(j-1, u) \\times b_q(x_{j+1}) \\times a_{u, v} \\times a_{v, q} \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "048fe137-261c-485a-86f1-5c7a721d0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def viterbi_algorithm(observations, states, start_probs, transition_probs, emission_probs, word_mapping):\n",
    "    \"\"\"\n",
    "    Runs the Viterbi algorithm to find the most likely sequence of hidden states.\n",
    "\n",
    "    INPUTS:\n",
    "    - observations (list): List of observed words/tokens.\n",
    "    - states (list): List of all possible states (sentiment labels).\n",
    "    - start_probs (np.array): Initial state probabilities.\n",
    "    - transition_probs (np.array): Transition probabilities matrix (state x state x state).\n",
    "    - emission_probs (np.array): Emission probabilities matrix (state x word).\n",
    "    - word_mapping (dict): Mapping of words to index in emissions probabiltiy np array\n",
    "\n",
    "    OUTPUTS:\n",
    "    - best_path (list): List of the most likely sequence of hidden states.\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    num_observations = len(observations)\n",
    "    print(num_states, num_observations)\n",
    "    \n",
    "    # Create a Viterbi matrix to store probabilities and backpointers\n",
    "    viterbi_matrix = np.zeros((num_states, num_observations))\n",
    "    backpointers = np.zeros((num_states, num_observations), dtype=int)\n",
    "    \n",
    "    # Initialize the first column of the Viterbi matrix using start probabilities\n",
    "    viterbi_matrix[:, 0] = start_probs * emission_probs[:, word_mapping[observations[0]]]\n",
    "    \n",
    "    # Fill in the Viterbi matrix and backpointers\n",
    "    for t in range(1, num_observations):\n",
    "        if observations[t] in word_mapping:\n",
    "            for s in range(num_states):\n",
    "                max_prob = -np.inf\n",
    "                max_prev_state = None\n",
    "                for prev_state_1 in range(num_states):\n",
    "                    for prev_state_2 in range(num_states):\n",
    "                        prob = viterbi_matrix[prev_state_2, t - 1] * transition_probs[prev_state_1, prev_state_2, s] * emission_probs[s, word_mapping[observations[t]]]\n",
    "                        if prob > max_prob:\n",
    "                            max_prob = prob\n",
    "                            max_prev_state = prev_state_2\n",
    "                viterbi_matrix[s, t] = max_prob\n",
    "                backpointers[s, t] = max_prev_state\n",
    "        \n",
    "    # Backtrack to find the best path\n",
    "    best_path = [0] * num_observations\n",
    "    best_path[-1] = np.argmax(viterbi_matrix[:, -1])\n",
    "    for t in range(num_observations - 2, 1, -1):\n",
    "        best_path[t] = backpointers[best_path[t + 1], t+1]\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "# Example usage\n",
    "ES_states_indexed = [i for i in range(len(ES_states))]\n",
    "RU_states_indexed = [i for i in range(len(RU_states))]\n",
    "# best_path_ES = viterbi_algorithm(ES_texts, ES_states_indexed, ES_start_probs, ES_a, ES_b, ES_wordmapping)\n",
    "# best_path_RU = viterbi_algorithm(RU_texts, RU_states_indexed , RU_start_probs, RU_a, RU_b, RU_wordmapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4228247-946c-4a99-b2eb-36ff883c2fa3",
   "metadata": {},
   "source": [
    "## 4.6 Model Evaluation\n",
    "Now that we've found our parameters $a_{u, v, q}, b_q(o)$, we can run it on the development, before saving the files to find their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43edb3c0-f96e-495f-a0e3-30804c29a485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 4312\n",
      "7 6589\n"
     ]
    }
   ],
   "source": [
    "# Data Processing for EU and RS\n",
    "ES_dev = \"ES/dev.in\"\n",
    "RU_dev = \"RU/dev.in\"\n",
    "\n",
    "dES_texts, dES_labels, dES_states, dES_state_mappings, dES_label_idx = process_data_HOHMM(ES_dev)\n",
    "dRU_texts, dRU_labels, dRU_states, dRU_state_mappings, dRU_label_idx = process_data_HOHMM(RU_dev)\n",
    "\n",
    "# Running viterbi using learned probabilities\n",
    "best_path_ES = viterbi_algorithm(dES_texts, ES_states_indexed, ES_start_probs, ES_a, ES_b, ES_wordmapping)\n",
    "best_path_RU = viterbi_algorithm(dRU_texts, RU_states_indexed , RU_start_probs, RU_a, RU_b, RU_wordmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08f643e8-be48-4d25-a26c-433349b8660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-neutral': 0,\n",
       " 'I-neutral': 1,\n",
       " 'B-negative': 2,\n",
       " 'B-positive': 3,\n",
       " 'I-negative': 4,\n",
       " 'O': 5,\n",
       " 'I-positive': 6}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RU_state_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca0546df-22d5-444d-817c-d31c76443290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to BIO labels\n",
    "def convert_labels(best_path, state_mappings):\n",
    "    for i in range(len(best_path)):\n",
    "        best_path[i] = state_mappings[best_path[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ee55d59-8c57-42ca-9f7d-bb15b6b1acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping of idx to states \n",
    "r_map = {}\n",
    "for k, v in RU_state_mappings.items():\n",
    "    r_map[v] = k\n",
    "convert_labels(best_path_ES, r_map)\n",
    "convert_labels(best_path_RU, r_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5fbfb1b-a4f2-427d-ba9d-8ebbbe05d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the files\n",
    "write_sequences_to_file(\"ES/dev.p4.out\", ES_texts, best_path_ES)\n",
    "write_sequences_to_file(\"RU/dev.p4.out\", RU_texts, best_path_RU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cdcb3-1a45-47ae-982b-4d6552ba5596",
   "metadata": {},
   "source": [
    "### RU\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 434\n",
    "\n",
    "#Correct Entity : 68\n",
    "Entity  precision: 0.1567\n",
    "Entity  recall: 0.1748\n",
    "Entity  F: 0.1652\n",
    "\n",
    "#Correct Sentiment : 0\n",
    "Sentiment  precision: 0.0000\n",
    "Sentiment  recall: 0.0000\n",
    "Sentiment  F: 0.0000\n",
    "\n",
    "\n",
    "### ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 252\n",
    "\n",
    "#Correct Entity : 18\n",
    "Entity  precision: 0.0714\n",
    "Entity  recall: 0.0786\n",
    "Entity  F: 0.0748\n",
    "\n",
    "#Correct Sentiment : 0\n",
    "Sentiment  precision: 0.0000\n",
    "Sentiment  recall: 0.0000\n",
    "Sentiment  F: 0.0000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f083876-67a1-40a2-8b8c-2b581e41654a",
   "metadata": {},
   "source": [
    "Unfortunately, as seen from the test results, it may appear that a HOHMM might not be underfitting in our context as the training set might be too small to optimally calculate robust higher order transmission probabilities. Sequences of 3 states in a row leads to $7^3 = 343$ possible combinations, up from the $7^2 = 49$ combinations that having a 1-order HMM would entail. This leads us to believe that our model is severely underfit, and that the 343 possbilities of transmission probabilties $a_{u, v, q}$ of which might be hard to cover given the small number of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06264c27-46f8-4798-ac53-ee526583e499",
   "metadata": {},
   "source": [
    "# 5.0 Design Challenge PART 2: Improvements to HOHMM\n",
    "As our team was dissatisfied with the results of HOHMM, we attempt another strategy in an attempt to improve upon the HOHMM model. In this part of the Design Challenge, we focus on using Laplace Smoothing.\n",
    "\n",
    "Laplace Smoothing involves adding a small constant (usually denoted as $ε$ or $k$) to the observed counts before calculating probabilities. This ensures that even events that haven't been observed in the training data are assigned nonzero probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec97de-4785-4c3c-b23b-4ca653eb7e65",
   "metadata": {},
   "source": [
    "## 5.1 Benefits of Laplace Smoothing\n",
    "1. Regularization of State Transitions:\n",
    "HOHMMs can have complex state transition structures. Since the model is underfitting, it might not be capturing the true relationships between states. By applying Laplace smoothing to transition probabilities, we ensure that even less frequent or unobserved transitions receive nonzero probabilities. This can help the model learn more accurate transition patterns and improve its performance.\n",
    "2. Dealing with Sparsity:\n",
    "In complex models like HOHMMs, some state transitions or hidden state emissions might have low counts or be completely unobserved in the training data. Laplace smoothing can address this sparsity issue by adding a small constant to the counts, preventing zero probabilities. This is particularly important in underfitting scenarios where the model might not be generalizing well to unseen or rare events.\n",
    "3. Improved Generalization:\n",
    "Underfitting often leads to poor generalization, where the model struggles to make accurate predictions on unseen data. Laplace smoothing encourages a more balanced distribution of probabilities, helping the model generalize better by assigning nonzero probabilities to a wider range of events and transitions.\n",
    "4. Balancing Model Complexity:\n",
    "Underfitting can sometimes occur when the model's complexity doesn't match the complexity of the data. Laplace smoothing can act as a form of regularization, balancing the model's complexity and promoting a smoother distribution of probabilities, which can help with more accurate modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d08bc-73e8-421f-a25d-138854f991ef",
   "metadata": {},
   "source": [
    "## 5.2 Laplace Smoothing for Transition and Emission Probabilities\n",
    "The smoothed transition probability from state $u, v$ to state $q$ is calculated as:\n",
    "$$\\text{Smoothed Transition Probability}(u, v \\to q) = \\frac{Count(u, v, q) + \\epsilon}{Count(u, v) + N_t \\times \\epsilon}$$\n",
    "wherbeby $N_t$ is the number of states.\n",
    "The smoothed emission probability of observing emission $b$ in state $q$ is calculated as:\n",
    "$$\\text{Smoothed Emission Probability}(q \\to o) = \\frac{Count(q  \\to o) + \\epsilon}{Count(q) + N_e \\times \\epsilon}$$\n",
    "wherbeby $N_e$ is the number of unique emissions.\n",
    "\n",
    "Hence, we modify the previous HOHMM functions to include $\\epsilon$, as well as take into account $N_e$ and $N_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5219b0c-8cfe-4d2a-bda6-a8e32d42e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emission_probs_laplace(texts, labels, states, epsilon):\n",
    "    \"\"\"\n",
    "    Calculates emission probabilities for a Higher Order Hidden Markov Model (HOHMM) + Laplace Smoothing\n",
    "\n",
    "    INPUTS:\n",
    "    - texts (list): List of observed words/tokens.\n",
    "    - labels (list): List of corresponding sentiment labels.\n",
    "    - states (list): List of all possible states (sentiment labels).\n",
    "    - epsilon (float): Smoothing constant\n",
    "\n",
    "    OUTPUTS:\n",
    "    - emission_probs (np.array): Emission probabilities matrix (state x word).\n",
    "    - words_to_idx (dict): Mappings from word to idx, following emissions probability np array\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    num_words = len(set(texts))\n",
    "    \n",
    "    # Emission probabilities (word given state)\n",
    "    emission_probs = np.zeros((num_states, num_words))\n",
    "    \n",
    "    # Count occurrences of words for each state\n",
    "    state_word_counts = np.zeros((num_states, num_words))\n",
    "\n",
    "    # Mapping dict\n",
    "    words_to_idx = {}\n",
    "    idx_to_words = {}\n",
    "    \n",
    "    # Process the dataset\n",
    "    for i in range(len(texts)):\n",
    "        state_idx = labels[i]  # Use label directly\n",
    "        word = texts[i]\n",
    "        word_idx = hash(word) % num_words  # Simplified hashing\n",
    "\n",
    "        words_to_idx[word] = word_idx\n",
    "        idx_to_words[word_idx] = word\n",
    "        state_word_counts[state_idx, word_idx] += 1\n",
    "    \n",
    "    # Calculate emission probabilities\n",
    "    for state_idx in range(num_states):\n",
    "        total_count = np.sum(state_word_counts[state_idx]) + num_words * epsilon # added epsilon to denom\n",
    "        emission_probs[state_idx] = (state_word_counts[state_idx] + epsilon) / total_count # added epsilon to numerator\n",
    "    \n",
    "    return emission_probs, words_to_idx\n",
    "\n",
    "\n",
    "def find_a_HOHMM_laplace(states, label_idx, epsilon):\n",
    "    \"\"\"\n",
    "    Finds a_uvq, HO transition probabilities\n",
    "    INPUTS:\n",
    "    - states (arr): array containing states in dataset\n",
    "    - label_udx (arr): Array of Indexed Labels for each corresponding text\n",
    "    - epsilon (float): Smoothing constant\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - a_uvq (np.array): np.array containign transition probabilities\n",
    "    \"\"\"\n",
    "    # Find number of states\n",
    "    num_states = len(states)\n",
    "    \n",
    "    # Initiate a np array to store HO a_uvq\n",
    "    transition_probs = np.zeros((num_states, num_states, num_states))\n",
    "    # Finding Count(u, v, q)\n",
    "    for i in range(2, len(label_idx)):\n",
    "        prev_state_1 = label_idx[i - 2]\n",
    "        prev_state_2 = label_idx[i - 1]\n",
    "        current_state = label_idx[i]\n",
    "        \n",
    "        transition_probs[prev_state_1, prev_state_2, current_state] += 1\n",
    "        \n",
    "    # Finding HO transition params\n",
    "    count_uv = np.nansum(transition_probs, axis=2, keepdims=True) + num_states * epsilon\n",
    "    a_uvq = (transition_probs + epsilon) / count_uv\n",
    "    \n",
    "    # Handle NaNs, convert to 0 probability\n",
    "    a_uvq = np.nan_to_num(a_uvq, nan=0.0)\n",
    "\n",
    "    return a_uvq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8d2d3-33f9-4fdc-9d97-363857d59f75",
   "metadata": {},
   "source": [
    "## 5.3 Retraining of Model\n",
    "Once again using `train.txt`, we try finding the new emission and transition probabilities using our new functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b660d667-81fa-4c2a-8408-1fc95018f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining initial epsilon\n",
    "epsilon = 1\n",
    "\n",
    "# Data Processing for EU and RS\n",
    "ES_train = \"ES/train.txt\"\n",
    "RU_train = \"RU/train.txt\"\n",
    "\n",
    "ES_texts, ES_labels, ES_states, ES_state_mappings, ES_label_idx = process_data_HOHMM(ES_train)\n",
    "RU_texts, RU_labels, RU_states, RU_state_mappings, RU_label_idx = process_data_HOHMM(RU_train)\n",
    "\n",
    "# Finding HO transition probabiltiies and emission for ES and RU, with smoothing\n",
    "ES_a = find_a_HOHMM_laplace(ES_states, ES_label_idx, epsilon)\n",
    "RU_a = find_a_HOHMM_laplace(RU_states, RU_label_idx, epsilon)\n",
    "\n",
    "RU_b, RU_wordmapping = calculate_emission_probs_laplace(RU_texts, RU_label_idx, RU_states, epsilon)\n",
    "ES_b, ES_wordmapping = calculate_emission_probs_laplace(ES_texts, ES_label_idx, ES_states, epsilon)\n",
    "\n",
    "# Calculating Start Probabilities\n",
    "ES_start_probs = calculate_start_probs(ES_label_idx, len(ES_states))\n",
    "RU_start_probs = calculate_start_probs(RU_label_idx, len(RU_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47e435-03da-4c36-bc6f-20c41a830726",
   "metadata": {},
   "source": [
    "## 5.4 Prediction and Evaluation\n",
    "We rerun the higher order Viterbi algorithm here before valuating it using the same metrics as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "713b85fb-1d25-43e4-bf0c-a81bb5556511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 4312\n",
      "7 6589\n"
     ]
    }
   ],
   "source": [
    "# Data Processing for EU and RS\n",
    "ES_dev = \"ES/dev.in\"\n",
    "RU_dev = \"RU/dev.in\"\n",
    "\n",
    "dES_texts, dES_labels, dES_states, dES_state_mappings, dES_label_idx = process_data_HOHMM(ES_dev)\n",
    "dRU_texts, dRU_labels, dRU_states, dRU_state_mappings, dRU_label_idx = process_data_HOHMM(RU_dev)\n",
    "\n",
    "# Running viterbi using learned probabilities\n",
    "best_path_ES = viterbi_algorithm(dES_texts, ES_states_indexed, ES_start_probs, ES_a, ES_b, ES_wordmapping)\n",
    "best_path_RU = viterbi_algorithm(dRU_texts, RU_states_indexed , RU_start_probs, RU_a, RU_b, RU_wordmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e25893b4-ff75-4db3-9c41-2da43ece3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping of idx to states \n",
    "r_map = {}\n",
    "for k, v in RU_state_mappings.items():\n",
    "    r_map[v] = k\n",
    "convert_labels(best_path_ES, r_map)\n",
    "convert_labels(best_path_RU, r_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd977a14-da41-4b59-882f-949cc1770112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the files\n",
    "write_sequences_to_file(\"ES/dev.p4.out\", ES_texts, best_path_ES)\n",
    "write_sequences_to_file(\"RU/dev.p4.out\", RU_texts, best_path_RU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "221e1295-de18-4d7b-94d9-5eb7f069e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the files\n",
    "write_sequences_to_file(\"P4 Tests/ESdev.p4.out\", ES_texts, best_path_ES)\n",
    "write_sequences_to_file(\"P4 Tests/RUdev.p4.out\", RU_texts, best_path_RU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39556f6-08c5-458e-9c62-c773bf607c6a",
   "metadata": {},
   "source": [
    "## RU\n",
    "\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 431\n",
    "\n",
    "#Correct Entity : 68\n",
    "Entity  precision: 0.1578\n",
    "Entity  recall: 0.1748\n",
    "Entity  F: 0.1659\n",
    "\n",
    "#Correct Sentiment : 0\n",
    "Sentiment  precision: 0.0000\n",
    "Sentiment  recall: 0.0000\n",
    "Sentiment  F: 0.0000\n",
    "\n",
    "## ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 248\n",
    "\n",
    "#Correct Entity : 17\n",
    "Entity  precision: 0.0685\n",
    "Entity  recall: 0.0742\n",
    "Entity  F: 0.0713\n",
    "\n",
    "#Correct Sentiment : 0\n",
    "Sentiment  precision: 0.0000\n",
    "Sentiment  recall: 0.0000\n",
    "Sentiment  F: 0.0000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526f13b-f41d-44ed-aa9d-9509442ef908",
   "metadata": {},
   "source": [
    "## 5.5 Conclusion to HOHMM\n",
    "There were insignificant changes to the HOHMM model, and we hereby conclude yet again that it was underfit. However, our team was still not satisfied and hence....,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f9c18d-d1a0-4e3f-9e1e-3b684b2aee12",
   "metadata": {},
   "source": [
    "# 6.0 Laplace Smoothing for the Initial HMM Model\n",
    "As we saw some intial success with the earlier HMM model, we decided to use Laplace Smoothing to regularise our parameters in order to attain a better result. \n",
    "\n",
    "In Part 1, we were told to add a constant to the emission probability calculation to take into account unknown values from future inputs, and that serves as a means of improvement. As Laplace Smoothing essentially does the same thing but with more benefits as mentioned above, we believe that we could continue taking away lesson from our failed HOHMM model and apply it to improve our intitial model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d3e4a-61c9-40ad-9a43-f501a1121237",
   "metadata": {},
   "source": [
    "## 6.1 Modifications of Functions\n",
    "The smoothed transition probability from state $u$ to state $v$ is calculated as:\n",
    "$$\\text{Smoothed Transition Probability}(u \\to v) = \\frac{Count(u, v) + \\epsilon}{Count(u) + N_t \\times \\epsilon}$$\n",
    "wherbeby $N_t$ is the number of states.\n",
    "The smoothed emission probability of observing emission $o$ in state $v$ is calculated as:\n",
    "$$\\text{Smoothed Emission Probability}(v \\to o) = \\frac{Count(v  \\to o) + \\epsilon}{Count(v) + N_e \\times \\epsilon}$$\n",
    "wherbeby $N_e$ is the number of unique emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80fdd0b3-2eb1-4761-a50b-03cda6a83ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_parameters_laplace(training_file_content, test_file_content, epsilon, Ne):\n",
    "    tags = {}  # Dictionary to store counts of each observation\n",
    "    emission_params = {}  # Dictionary to store emission probabilities\n",
    "    train_words = []\n",
    "    test_words = []\n",
    "    \n",
    "    # Split the file content into lines\n",
    "    train_data_lines = training_file_content.strip().split(\"\\n\")\n",
    "    test_data_lines = test_file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line in train_data_lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if ' O' in line:\n",
    "            observation, tag = line[:-2], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-11], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-10], line[-9:]\n",
    "        if observation not in train_words:\n",
    "            train_words.append(observation)\n",
    "        if tag in tags:\n",
    "            tags[tag][observation] = tags[tag].get(observation, 0) + 1\n",
    "        else:\n",
    "            tags[tag] = {observation: 1}\n",
    "    # Iterate through each line to extract observations from test set\n",
    "    for line in test_data_lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if ' O' in line:\n",
    "            observation, tag = line[:-2], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-11], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-10], line[-9:]\n",
    "        else:\n",
    "            observation = line\n",
    "        if observation not in test_words:\n",
    "            test_words.append(observation)\n",
    "\n",
    "    # Calculate emission probabilities for each observation and tag\n",
    "    for tag, observations_count in tags.items():\n",
    "        total_count = sum(observations_count.values())\n",
    "        emission_params[tag] = {\n",
    "            observation: (count + epsilon) / (total_count + Ne * epsilon) for observation, count in observations_count.items()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    return emission_params, list(set(test_words).union(train_words))\n",
    "# Modified Transmission Parameters with Laplace\n",
    "\n",
    "def estimate_transition_parameters_laplace(file_content, epsilon):\n",
    "    transition_counts = {}\n",
    "    transition_params = {}  # Dictionary to store emission probabilities\n",
    "\n",
    "    # Split the file content into lines\n",
    "    lines = file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Nt number of labels\n",
    "    Nt = 7\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line_index in range(len(lines)):\n",
    "        next_line = \"\"\n",
    "        curr_line = lines[line_index]\n",
    "        if line_index < len(lines) - 1:\n",
    "            next_line = lines[line_index + 1]\n",
    "        \n",
    "        if len(curr_line) == 0:\n",
    "            continue\n",
    "        if curr_line[-1] == 'O':\n",
    "            curr_observation, curr_tag = curr_line[:-1], curr_line[-1]\n",
    "        elif ('B-positive' in curr_line \n",
    "            or 'I-positive' in curr_line \n",
    "            or 'B-negative' in curr_line \n",
    "            or 'I-negative' in curr_line):\n",
    "            curr_observation, curr_tag = curr_line[:-10], curr_line[-10:]\n",
    "        elif ('B-neutral' in curr_line\n",
    "            or 'I-neutral' in curr_line):\n",
    "            curr_observation, curr_tag = curr_line[:-9], curr_line[-9:]\n",
    "        \n",
    "        if len(next_line) == 0: #true if line_index = len(lines - 1) or line_index is last index of a document\n",
    "            pass\n",
    "        elif next_line[-1] == 'O':\n",
    "            next_observation, next_tag = next_line[:-1], next_line[-1]\n",
    "        elif ('B-positive' in next_line \n",
    "            or 'I-positive' in next_line \n",
    "            or 'B-negative' in next_line \n",
    "            or 'I-negative' in next_line):\n",
    "            next_observation, next_tag = next_line[:-10], next_line[-10:]\n",
    "        elif ('B-neutral' in next_line\n",
    "            or 'I-neutral' in next_line):\n",
    "            next_observation, next_tag = next_line[:-9], next_line[-9:]        \n",
    "        \n",
    "        # Handle transitions from START\n",
    "        if line_index == 0 or lines[line_index - 1] == \"\":\n",
    "            if 'START' in transition_counts:\n",
    "                transition_counts['START'][curr_tag] = transition_counts['START'].get(curr_tag, 0) + 1\n",
    "            else:\n",
    "                transition_counts['START'] = {curr_tag: 1}\n",
    "                \n",
    "        # Handle transitions to STOP\n",
    "        if len(next_line) == 0:\n",
    "            if curr_tag in transition_counts:\n",
    "                transition_counts[curr_tag]['STOP'] = transition_counts[curr_tag].get('STOP', 0) + 1\n",
    "            else:\n",
    "                transition_counts[curr_tag] = {'STOP': 1}\n",
    "                \n",
    "        # Rest of the transitions\n",
    "        else:\n",
    "            if curr_tag in transition_counts:\n",
    "                transition_counts[curr_tag][next_tag] = transition_counts[curr_tag].get(next_tag, 0) + 1\n",
    "            else:\n",
    "                transition_counts[curr_tag] = {next_tag: 1}\n",
    "        \n",
    "\n",
    "    # Calculate transition probabilities \n",
    "    for tag, next_tag_counts in transition_counts.items():\n",
    "        total_count = sum(next_tag_counts.values()) \n",
    "        transition_params[tag] = {\n",
    "            next_tag: (count + epsilon) / (total_count + Nt * epsilon) for next_tag, count in next_tag_counts.items()\n",
    "        }\n",
    "\n",
    "    return transition_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628757c8-c2ff-4151-a85a-edb1b9391468",
   "metadata": {},
   "source": [
    "## 6.2 Training the Initial Model with Laplace\n",
    "Leveraging upon functions we wrote too for HOHMM in order to calculate `Ne`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf0c22bd-6fd1-4858-93b4-cfe44b870a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing for EU and RS\n",
    "ES_train = \"ES/train.txt\"\n",
    "RU_train = \"RU/train.txt\"\n",
    "\n",
    "ES_texts, _, _, _, _ = process_data_HOHMM(ES_train)\n",
    "RU_texts, _, _, _, _ = process_data_HOHMM(RU_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a49eae1-ec92-4464-9a37-ec562665e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "\n",
    "#ES\n",
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "Ne = len(set(ES_texts))\n",
    "emission_params = estimate_emission_parameters_laplace(training_file_content, test_file_content, epsilon, Ne)[0]\n",
    "transition_params = estimate_transition_parameters_laplace(training_file_content, epsilon)\n",
    "observation_sequences = create_observation_sequences(\"ES/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"P4 Tests/ESdev.p4.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d14d23d-3086-4448-b99f-3dfd0b47a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RU\n",
    "training_file_content = read_file(\"RU/train.txt\")\n",
    "test_file_content = read_file(\"RU/dev.in\")\n",
    "Ne = len(set(RU_texts))\n",
    "emission_params = estimate_emission_parameters_laplace(training_file_content, test_file_content, epsilon, Ne)[0]\n",
    "transition_params = estimate_transition_parameters_laplace(training_file_content, epsilon)\n",
    "observation_sequences = create_observation_sequences(\"RU/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"P4 Tests/RUdev.p4.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aeb897-8b18-4251-bfc7-f1eb59867feb",
   "metadata": {},
   "source": [
    "## 6.3 Evaluation of using Laplace on the Initial HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a51af5-1997-4fae-b6ce-e6eee1685dfa",
   "metadata": {},
   "source": [
    "### ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 75\n",
    "\n",
    "#Correct Entity : 67\n",
    "Entity  precision: 0.8933\n",
    "Entity  recall: 0.2926\n",
    "Entity  F: 0.4408\n",
    "\n",
    "#Correct Sentiment : 55\n",
    "Sentiment  precision: 0.7333\n",
    "Sentiment  recall: 0.2402\n",
    "Sentiment  F: 0.3618\n",
    "\n",
    "### RU\n",
    "python3 evalResult.py RUdev.out RUdev.p4.out \n",
    "\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 3867\n",
    "\n",
    "#Correct Entity : 208\n",
    "Entity  precision: 0.0538\n",
    "Entity  recall: 0.5347\n",
    "Entity  F: 0.0977\n",
    "\n",
    "#Correct Sentiment : 150\n",
    "Sentiment  precision: 0.0388\n",
    "Sentiment  recall: 0.3856\n",
    "Sentiment  F: 0.0705"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07991ac-2ea6-49fe-bbb4-53f20b1b1a68",
   "metadata": {},
   "source": [
    "As we can see, there are significant improvements to the F score of the ES set by using Laplace Smoothing. We hencefourth stick to the use of Smoothing Laplacing for it, incresaing the epsilon parameter further to try and improve our robustness on the developments set till we see no further significant improvements.\n",
    "\n",
    "However, the RU set seems to experience a huge decrease in Fscore, and hence we don't use Laplace Smoothing for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dc555-1cb3-4b29-bff2-c4d6ca9db7f5",
   "metadata": {},
   "source": [
    "## 6.4 Improving the $\\epsilon$ Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7de89901-4d84-4a91-898a-50365df6e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon in range(2, 20, 2):\n",
    "    # ES\n",
    "    training_file_content = read_file(\"ES/train.txt\")\n",
    "    test_file_content = read_file(\"ES/dev.in\")\n",
    "    Ne = len(set(ES_texts))\n",
    "    emission_params = estimate_emission_parameters_laplace(training_file_content, test_file_content, epsilon, Ne)[0]\n",
    "    transition_params = estimate_transition_parameters_laplace(training_file_content, epsilon)\n",
    "    observation_sequences = create_observation_sequences(\"ES/dev.in\")\n",
    "    best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "    # print(best_sequences)\n",
    "       \n",
    "    write_sequences_to_file(f\"P4 Tests/ESdev{epsilon}.p4.out\", observation_sequences, best_sequences)\n",
    "    \"\"\"\n",
    "    # RU\n",
    "    training_file_content = read_file(\"RU/train.txt\")\n",
    "    test_file_content = read_file(\"RU/dev.in\")\n",
    "    Ne = len(set(RU_texts))\n",
    "    emission_params = estimate_emission_parameters_laplace(training_file_content, test_file_content, epsilon, Ne)[0]\n",
    "    transition_params = estimate_transition_parameters_laplace(training_file_content, epsilon)\n",
    "    observation_sequences = create_observation_sequences(\"RU/dev.in\")\n",
    "    best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "    # print(best_sequences)\n",
    "       \n",
    "    write_sequences_to_file(f\"P4 Tests/RUdev{epsilon}.p4.out\", observation_sequences, best_sequences)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d82168-68d0-4666-ad60-1ba14507d24c",
   "metadata": {},
   "source": [
    "Here are the results:\n",
    "#### $\\epsilon = 2$\n",
    "\n",
    "python3 evalResult.py ESdev.out ESdev2.p4.out\n",
    "\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 49\n",
    "\n",
    "#Correct Entity : 42\n",
    "Entity  precision: 0.8571\n",
    "Entity  recall: 0.1834\n",
    "Entity  F: 0.3022\n",
    "\n",
    "#Correct Sentiment : 34\n",
    "Sentiment  precision: 0.6939\n",
    "Sentiment  recall: 0.14\n",
    "\n",
    "#### $\\epsilon = 4$\n",
    "\n",
    "python3 evalResult.py ESdev.out ESdev2.p4.out\n",
    "\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 49\n",
    "\n",
    "#Correct Entity : 42\n",
    "Entity  precision: 0.8571\n",
    "Entity  recall: 0.1834\n",
    "Entity  F: 0.3022\n",
    "\n",
    "#Correct Sentiment : 34\n",
    "Sentiment  precision: 0.6939\n",
    "Sentiment  recall: 0.1485\n",
    "Sentiment  F: 0.2446\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc5eee58-8e1b-4125-a737-b28492488f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.7\n",
    "\n",
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "Ne = len(set(ES_texts))\n",
    "emission_params = estimate_emission_parameters_laplace(training_file_content, test_file_content, epsilon, Ne)[0]\n",
    "transition_params = estimate_transition_parameters_laplace(training_file_content, epsilon)\n",
    "observation_sequences = create_observation_sequences(\"ES/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(f\"P4 Tests/ESdev{epsilon}.p4.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c52fe-7616-4209-bcee-f7f629053115",
   "metadata": {},
   "source": [
    "python3 evalResult.py ESdev.out ESdev0.7.p4.out\n",
    "\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 77\n",
    "\n",
    "#Correct Entity : 69\n",
    "Entity  precision: 0.8961\n",
    "Entity  recall: 0.3013\n",
    "Entity  F: 0.4510\n",
    "\n",
    "#Correct Sentiment : 57\n",
    "Sentiment  precision: 0.7403\n",
    "Sentiment  recall: 0.2489\n",
    "Sentiment  F: 0.3725\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31f15ef2-8bf5-45e1-9de4-06a79aac1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.4\n",
    "\n",
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "Ne = len(set(ES_texts))\n",
    "emission_params = estimate_emission_parameters_laplace(training_file_content, test_file_content, epsilon, Ne)[0]\n",
    "transition_params = estimate_transition_parameters_laplace(training_file_content, epsilon)\n",
    "observation_sequences = create_observation_sequences(\"ES/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(f\"P4 Tests/ESdev{epsilon}.p4.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78b3ad-0100-461c-afe6-b87d5e24cc9c",
   "metadata": {},
   "source": [
    "python3 evalResult.py ESdev.out ESdev0.4.p4.out\n",
    "\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 92\n",
    "\n",
    "#Correct Entity : 82\n",
    "Entity  precision: 0.8913\n",
    "Entity  recall: 0.3581\n",
    "Entity  F: 0.5109\n",
    "\n",
    "#Correct Sentiment : 67\n",
    "Sentiment  precision: 0.7283\n",
    "Sentiment  recall: 0.2926\n",
    "Sentiment  F: 0.4174\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28a74d-0ab9-4548-9d46-0684aa004c99",
   "metadata": {},
   "source": [
    "As there seems to be an overall loss in the ES set's performance using Laplace Smoothing, it could be that the initial model was already fitted enough to handle data. As such we keep it as it is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134fb54-bf1c-4a93-a18e-915646724a4a",
   "metadata": {},
   "source": [
    "## 6.5 Saving the Final Models\n",
    "We save the develpoment results here from our final model choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c62ce99-9145-4989-8e59-ee975dbb2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES with Epsilon = 1\n",
    "epsilon = 1\n",
    "training_file_content = read_file(\"RU/train.txt\")\n",
    "test_file_content = read_file(\"RU/dev.in\")\n",
    "Ne = len(set(RU_texts))\n",
    "emission_params = estimate_emission_parameters_laplace(training_file_content, test_file_content, epsilon, Ne)[0]\n",
    "transition_params = estimate_transition_parameters_laplace(training_file_content, epsilon)\n",
    "observation_sequences = create_observation_sequences(\"RU/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"RU/dev.p4.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4640609-6366-4fbc-98b5-c112bbbf5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial RU\n",
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "emission_params = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "transition_params = estimate_transition_parameters(training_file_content)\n",
    "observation_sequences = create_observation_sequences(\"ES/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"ES/dev.p4.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1cb9e-1371-4b9a-a464-2a3ea9750cc4",
   "metadata": {},
   "source": [
    "## 6.6. Running on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "363c35cb-0849-4805-bb27-b88e192e8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RU with Epsilon = 1\n",
    "epsilon = 1.0\n",
    "\n",
    "#ES\n",
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"Test/ES/test.in\")\n",
    "Ne = len(set(ES_texts))\n",
    "emission_params = estimate_emission_parameters_laplace(training_file_content, test_file_content, epsilon, Ne)[0]\n",
    "transition_params = estimate_transition_parameters_laplace(training_file_content, epsilon)\n",
    "observation_sequences = create_observation_sequences(\"Test/ES/test.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"P4 Tests/EStest.p4.out\", observation_sequences, best_sequences)\n",
    "write_sequences_to_file(\"ES/test.p4.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d43ca76-8455-440e-b1a4-ae902f90ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial ES\n",
    "training_file_content = read_file(\"RU/train.txt\")\n",
    "test_file_content = read_file(\"Test/RU/test.in\")\n",
    "emission_params = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "transition_params = estimate_transition_parameters(training_file_content)\n",
    "observation_sequences = create_observation_sequences(\"Test/RU/test.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "\n",
    "write_sequences_to_file(\"P4 Tests/RUtest.p4.out\", observation_sequences, best_sequences)\n",
    "write_sequences_to_file(\"RU/test.p4.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3a32a-d91e-4b63-b664-ea7f49dc081d",
   "metadata": {},
   "source": [
    "Thanks for joining us on this arduous process of model improvement! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303469a4-6c2b-4bbc-8ee7-babe1b5a0562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
