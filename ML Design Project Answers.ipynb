{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d838c1d",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7590be0",
   "metadata": {},
   "source": [
    "Write a function that estimates the emission parameters from the training set using MLE (maximum\n",
    "likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b373d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        file_content = file.read()\n",
    "    return file_content\n",
    "\n",
    "def write_file(file_path, content):\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5331aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_parameters(file_content):\n",
    "    tags = {}  # Dictionary to store counts of each observation\n",
    "    emission_params = {}  # Dictionary to store emission probabilities\n",
    "\n",
    "    # Split the file content into lines\n",
    "    lines = file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if line[-1] == 'O':\n",
    "            observation, tag = line[:-1], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-10], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-9], line[-9:]\n",
    "#         observation, tag = line.split()\n",
    "    \n",
    "        if tag in tags:\n",
    "            tags[tag][observation] = tags[tag].get(observation, 0) + 1\n",
    "        else:\n",
    "            tags[tag] = {observation: 1}\n",
    "\n",
    "    # Calculate emission probabilities for each observation and tag\n",
    "    for tag, observations_count in tags.items():\n",
    "        total_count = sum(observations_count.values())\n",
    "        emission_params[tag] = {\n",
    "            observation: count / total_count for observation, count in observations_count.items()\n",
    "        }\n",
    "\n",
    "    return emission_params\n",
    "\n",
    "file_content = read_file(\"ES/train.txt\")\n",
    "\n",
    "emission_parameters = estimate_emission_parameters(file_content)\n",
    "# print(emission_parameters, sum(len(words) for words in emission_parameters.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30067806",
   "metadata": {},
   "source": [
    "One problem with estimating the emission parameters is that some words that appear in the test set\n",
    "do not appear in the training set. One simple idea to handle this issue is as follows. We introduce\n",
    "a special word token #UNK#, and make the following modifications to the computation of emission\n",
    "probabilities.During the testing phase, if the word does not appear in the training set, we replace that word with\n",
    "#UNK#. Set k to 1, implement this fix into your function for computing the emission parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b13646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_parameters_modified(training_file_content, test_file_content, k=1):\n",
    "    tags = {}  # Dictionary to store counts of each observation\n",
    "    emission_params = {}  # Dictionary to store emission probabilities\n",
    "    train_words = []\n",
    "    test_words = []\n",
    "    \n",
    "    # Split the file content into lines\n",
    "    train_data_lines = training_file_content.strip().split(\"\\n\")\n",
    "    test_data_lines = test_file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line in train_data_lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if ' O' in line:\n",
    "            observation, tag = line[:-2], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-11], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-10], line[-9:]\n",
    "        if observation not in train_words:\n",
    "            train_words.append(observation)\n",
    "        if tag in tags:\n",
    "            tags[tag][observation] = tags[tag].get(observation, 0) + 1\n",
    "        else:\n",
    "            tags[tag] = {observation: 1}\n",
    "    # Iterate through each line to extract observations from test set\n",
    "    for line in test_data_lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if ' O' in line:\n",
    "            observation, tag = line[:-2], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-11], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-10], line[-9:]\n",
    "        else:\n",
    "            observation = line\n",
    "        if observation not in test_words:\n",
    "            test_words.append(observation)\n",
    "    # Extract words that are in test set but not in train set   \n",
    "    unique_words = find_unique_words(test_words, train_words)\n",
    "\n",
    "    # Calculate emission probabilities for each observation and tag\n",
    "    for tag, observations_count in tags.items():\n",
    "        total_count = sum(observations_count.values())\n",
    "        emission_params[tag] = {\n",
    "            observation: count / (total_count + k) for observation, count in observations_count.items()\n",
    "        }\n",
    "        for word in unique_words:\n",
    "            emission_params[tag][word] = k / (total_count + k)\n",
    "\n",
    "\n",
    "    return emission_params, list(set(test_words).union(train_words))\n",
    "\n",
    "def find_unique_words(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    unique_in_list1 = set1 - set2\n",
    "\n",
    "    return list(unique_in_list1)\n",
    "\n",
    "\n",
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "emission_parameters = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "# print(emission_parameters, sum(len(words) for words in emission_parameters.values()))\n",
    "count = 0\n",
    "for emission_probabilities in emission_parameters.values():\n",
    "    count += len(emission_probabilities)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c12afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(path_to_train_file, path_to_test_file):\n",
    "    training_file_content = read_file(path_to_train_file)\n",
    "    test_file_content = read_file(path_to_test_file)\n",
    "    emission_parameters, words = estimate_emission_parameters_modified(training_file_content, test_file_content)\n",
    "    word_to_label = {}\n",
    "    for word in words:\n",
    "        probabilities = []\n",
    "        for label, freqs in emission_parameters.items():\n",
    "            if word in freqs:\n",
    "                probabilities.append((label, freqs[word]))\n",
    "        word_to_label[word] = max(probabilities, key=lambda x: x[1])[0]\n",
    "    return word_to_label\n",
    "\n",
    "def write_result_to_file(word_to_label, path_to_dev_set, path_to_output):\n",
    "    result = \"\"\n",
    "    dev_file_content = read_file(path_to_dev_set)\n",
    "    dev_set_lines = dev_file_content.strip().split(\"\\n\")\n",
    "    counter = 0\n",
    "    for line in dev_set_lines:\n",
    "        if line == \"\":\n",
    "            result += \"\\n\"\n",
    "            continue  \n",
    "        result += line + \" \" + word_to_label.get(line) + \"\\n\"\n",
    " \n",
    "    write_file(path_to_output, result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b9f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = sentiment_analysis(\"ES/train.txt\", \"ES/dev.in\")\n",
    "write_result_to_file(mapping, \"ES/dev.in\", \"ES/dev.p1.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03317db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = sentiment_analysis(\"RU/train.txt\", \"RU/dev.in\")\n",
    "write_result_to_file(mapping, \"RU/dev.in\", \"RU/dev.p1.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18d25a",
   "metadata": {},
   "source": [
    "## ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 1466\n",
    "\n",
    "#Correct Entity : 178\n",
    "Entity  precision: 0.1214\n",
    "Entity  recall: 0.7773\n",
    "Entity  F: 0.2100\n",
    "\n",
    "#Correct Sentiment : 97\n",
    "Sentiment  precision: 0.0662\n",
    "Sentiment  recall: 0.4236\n",
    "Sentiment  F: 0.1145\n",
    "\n",
    "## RU\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 1816\n",
    "\n",
    "#Correct Entity : 266\n",
    "Entity  precision: 0.1465\n",
    "Entity  recall: 0.6838\n",
    "Entity  F: 0.2413\n",
    "\n",
    "#Correct Sentiment : 129\n",
    "Sentiment  precision: 0.0710\n",
    "Sentiment  recall: 0.3316\n",
    "Sentiment  F: 0.1170"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64518f2",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cb2b5",
   "metadata": {},
   "source": [
    "Write a function that estimates the transition parameters from the training set using MLE (maximum\n",
    "likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62408ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'START': {'O': 0.9289176090468497, 'B-positive': 0.052234787291330104, 'B-negative': 0.014001077005923533, 'B-neutral': 0.004846526655896607}, 'O': {'O': 0.8856896848630963, 'B-positive': 0.03650766316514551, 'STOP': 0.06344067504735663, 'B-negative': 0.012226623041157224, 'B-neutral': 0.0021353538832443605}, 'B-positive': {'O': 0.871551724137931, 'I-positive': 0.11637931034482758, 'STOP': 0.008620689655172414, 'B-neutral': 0.0008620689655172414, 'B-positive': 0.002586206896551724}, 'B-negative': {'O': 0.8110236220472441, 'I-negative': 0.1784776902887139, 'STOP': 0.010498687664041995}, 'B-neutral': {'I-neutral': 0.20833333333333334, 'O': 0.7916666666666666}, 'I-neutral': {'I-neutral': 0.6511627906976745, 'O': 0.3488372093023256}, 'I-positive': {'I-positive': 0.5700636942675159, 'O': 0.4267515923566879, 'STOP': 0.0031847133757961785}, 'I-negative': {'O': 0.39766081871345027, 'I-negative': 0.6023391812865497}}\n"
     ]
    }
   ],
   "source": [
    "def estimate_transition_parameters(file_content):\n",
    "    transition_counts = {}\n",
    "    transition_params = {}  # Dictionary to store emission probabilities\n",
    "\n",
    "    # Split the file content into lines\n",
    "    lines = file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line_index in range(len(lines)):\n",
    "        next_line = \"\"\n",
    "        curr_line = lines[line_index]\n",
    "        if line_index < len(lines) - 1:\n",
    "            next_line = lines[line_index + 1]\n",
    "        \n",
    "        if len(curr_line) == 0:\n",
    "            continue\n",
    "        if curr_line[-1] == 'O':\n",
    "            curr_observation, curr_tag = curr_line[:-1], curr_line[-1]\n",
    "        elif ('B-positive' in curr_line \n",
    "            or 'I-positive' in curr_line \n",
    "            or 'B-negative' in curr_line \n",
    "            or 'I-negative' in curr_line):\n",
    "            curr_observation, curr_tag = curr_line[:-10], curr_line[-10:]\n",
    "        elif ('B-neutral' in curr_line\n",
    "            or 'I-neutral' in curr_line):\n",
    "            curr_observation, curr_tag = curr_line[:-9], curr_line[-9:]\n",
    "        \n",
    "        if len(next_line) == 0: #true if line_index = len(lines - 1) or line_index is last index of a document\n",
    "            pass\n",
    "        elif next_line[-1] == 'O':\n",
    "            next_observation, next_tag = next_line[:-1], next_line[-1]\n",
    "        elif ('B-positive' in next_line \n",
    "            or 'I-positive' in next_line \n",
    "            or 'B-negative' in next_line \n",
    "            or 'I-negative' in next_line):\n",
    "            next_observation, next_tag = next_line[:-10], next_line[-10:]\n",
    "        elif ('B-neutral' in next_line\n",
    "            or 'I-neutral' in next_line):\n",
    "            next_observation, next_tag = next_line[:-9], next_line[-9:]        \n",
    "        \n",
    "        # Handle transitions from START\n",
    "        if line_index == 0 or lines[line_index - 1] == \"\":\n",
    "            if 'START' in transition_counts:\n",
    "                transition_counts['START'][curr_tag] = transition_counts['START'].get(curr_tag, 0) + 1\n",
    "            else:\n",
    "                transition_counts['START'] = {curr_tag: 1}\n",
    "                \n",
    "        # Handle transitions to STOP\n",
    "        if len(next_line) == 0:\n",
    "            if curr_tag in transition_counts:\n",
    "                transition_counts[curr_tag]['STOP'] = transition_counts[curr_tag].get('STOP', 0) + 1\n",
    "            else:\n",
    "                transition_counts[curr_tag] = {'STOP': 1}\n",
    "                \n",
    "        # Rest of the transitions\n",
    "        else:\n",
    "            if curr_tag in transition_counts:\n",
    "                transition_counts[curr_tag][next_tag] = transition_counts[curr_tag].get(next_tag, 0) + 1\n",
    "            else:\n",
    "                transition_counts[curr_tag] = {next_tag: 1}\n",
    "        \n",
    "\n",
    "    # Calculate transition probabilities \n",
    "    for tag, next_tag_counts in transition_counts.items():\n",
    "        total_count = sum(next_tag_counts.values())\n",
    "        transition_params[tag] = {\n",
    "            next_tag: count / total_count for next_tag, count in next_tag_counts.items()\n",
    "        }\n",
    "\n",
    "    return transition_params\n",
    "\n",
    "file_content = read_file(\"ES/train.txt\")\n",
    "\n",
    "transition_parameters = estimate_transition_parameters(file_content)\n",
    "print(transition_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181cb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(observation_sequence, emission_params, transition_params):\n",
    "    states = list(emission_params.keys())\n",
    "    T = len(observation_sequence)\n",
    "    N = len(states)\n",
    "\n",
    "    # Initialization step\n",
    "    viterbi_table = [{}]\n",
    "    backpointer = [{}]\n",
    "    for state in states:\n",
    "        viterbi_table[0][state] = transition_params['START'].get(state, 0) * emission_params[state].get(observation_sequence[0], 0)\n",
    "        backpointer[0][state] = None\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, T):\n",
    "        viterbi_table.append({})\n",
    "        backpointer.append({})\n",
    "        for state in states:\n",
    "            max_prob = max(\n",
    "                viterbi_table[t - 1][prev_state] * transition_params[prev_state].get(state, 0) * emission_params[state].get(observation_sequence[t], 0)\n",
    "                for prev_state in states\n",
    "            )\n",
    "            viterbi_table[t][state] = max_prob\n",
    "            backpointer[t][state] = max(states, key=lambda prev_state: viterbi_table[t - 1][prev_state] * transition_params[prev_state].get(state, 0))\n",
    "\n",
    "    # Termination step\n",
    "    max_prob_last_state = max(viterbi_table[T - 1].values())\n",
    "    best_last_state = max(states, key=lambda state: viterbi_table[T - 1][state] * transition_params[state].get('STOP', 0))\n",
    "\n",
    "    # Backtrack to find the best sequence\n",
    "    best_sequence = [best_last_state]\n",
    "    prev_state = best_last_state\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        best_sequence.insert(0, backpointer[t + 1][prev_state])\n",
    "        prev_state = backpointer[t + 1][prev_state]\n",
    "\n",
    "    return best_sequence\n",
    "\n",
    "def create_observation_sequences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        file_content = file.read().strip()\n",
    "\n",
    "    observation_sequences = []\n",
    "    current_sequence = []\n",
    "\n",
    "    # Split the file content by empty lines to get individual sequences\n",
    "    sequence_lines = file_content.split(\"\\n\\n\")\n",
    "\n",
    "    for sequence_line in sequence_lines:\n",
    "        # Split each sequence into individual observations (words)\n",
    "        observation_sequence = sequence_line.strip().split(\"\\n\")\n",
    "        current_sequence.extend(observation_sequence)\n",
    "        observation_sequences.append(current_sequence)\n",
    "        current_sequence = []\n",
    "\n",
    "    return observation_sequences\n",
    "\n",
    "def viterbi_for_sequences(observation_sequences, emission_params, transition_params):\n",
    "    best_sequences = []\n",
    "    for observation_sequence in observation_sequences:\n",
    "        best_sequence = viterbi(observation_sequence, emission_params, transition_params)\n",
    "        best_sequences.append(best_sequence)\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "def write_sequences_to_file(output_file_path, observation_sequences, best_sequences):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for obs_sequence, best_sequence in zip(observation_sequences, best_sequences):\n",
    "            for word, tag in zip(obs_sequence, best_sequence):\n",
    "                file.write(f\"{word} {tag}\\n\")\n",
    "            file.write(\"\\n\")  # Separate sequences with an empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a315bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "emission_params = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "transition_params = estimate_transition_parameters(training_file_content)\n",
    "observation_sequences = create_observation_sequences(\"ES/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"ES/dev.p2.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dac0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_content = read_file(\"RU/train.txt\")\n",
    "test_file_content = read_file(\"RU/dev.in\")\n",
    "emission_params = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "transition_params = estimate_transition_parameters(training_file_content)\n",
    "observation_sequences = create_observation_sequences(\"RU/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"RU/dev.p2.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56d1b4",
   "metadata": {},
   "source": [
    "## ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 542\n",
    "\n",
    "#Correct Entity : 134\n",
    "Entity  precision: 0.2472\n",
    "Entity  recall: 0.5852\n",
    "Entity  F: 0.3476\n",
    "\n",
    "#Correct Sentiment : 97\n",
    "Sentiment  precision: 0.1790\n",
    "Sentiment  recall: 0.4236\n",
    "Sentiment  F: 0.2516"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d21ca7",
   "metadata": {},
   "source": [
    "## RU\n",
    "\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 514\n",
    "\n",
    "#Correct Entity : 190\n",
    "Entity  precision: 0.3696\n",
    "Entity  recall: 0.4884\n",
    "Entity  F: 0.4208\n",
    "\n",
    "#Correct Sentiment : 129\n",
    "Sentiment  precision: 0.2510\n",
    "Sentiment  recall: 0.3316\n",
    "Sentiment  F: 0.2857"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875a221-d46d-460a-a531-c516476069e6",
   "metadata": {},
   "source": [
    "# Part 4: Design Challenge\n",
    "For our design, we propose the use of Higher Order Hidden Markov Models (HOHMM). \n",
    "In a Higher Order HMM, emission pand transition probabilities are conditioned on the current state and the previous several states. This allows the model to capture longer-range dependencies in the sequence. For sentiment analysis, this could help capture more complex sentiment patterns that extend over multiple words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab635ea-c0d8-4528-9f80-26fa06bf33ac",
   "metadata": {},
   "source": [
    "## 4.1 Data Processing\n",
    "Here, we aim to use integers to index each possible state in order. This is done so as to enable the use of numpy arrays for calculating of Higher-order Transition Probabilities, that take into account not just the previous state but one before it that will be done in section `4.2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3583ba2e-fe32-43a6-bdf2-017f63c2345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "a5382e56-e337-46ec-a674-e304de702546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for this step 4.1\n",
    "def read_data_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Seperates txt file into texts and labels arary\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                text = parts[0]\n",
    "                label = parts[1]\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "            elif len(parts) == 1:\n",
    "                texts.append(line.strip())\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "def process_data_HOHMM(file_path):\n",
    "    \"\"\"\n",
    "    Finds higher order transition probabilities\n",
    "    INPUTS:\n",
    "    - file_path (string): Path to the data file in txt format\n",
    "    OUTPUTS:\n",
    "    - texts: array containing all texts\n",
    "    - labels: array containing coressponding labels of each text\n",
    "    - states: array containing unique states\n",
    "    - state_to_idx: mapping of states to a numerical index\n",
    "    - label_idx: array of states in numerical idx, corresponding to texts\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    texts, labels = read_data_from_file(file_path)\n",
    "    # Get states\n",
    "    states = list(set(labels))\n",
    "    \n",
    "    # Define a mapping from states to indices\n",
    "    state_to_idx = {state: idx for idx, state in enumerate(states)}\n",
    "    \n",
    "    # Get array of state indices corresponding to each text in the texts array\n",
    "    label_idx = [state_to_idx[label] for label in labels]\n",
    "\n",
    "    return texts, labels, states, state_to_idx, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "acb8b9f0-e3c2-4d6f-970c-aa80c1462ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing for EU and RS\n",
    "ES_train = \"ES/train.txt\"\n",
    "RU_train = \"RU/train.txt\"\n",
    "\n",
    "ES_texts, ES_labels, ES_states, ES_state_mappings, ES_label_idx = process_data_HOHMM(ES_train)\n",
    "RU_texts, RU_labels, RU_states, RU_state_mappings, RU_label_idx = process_data_HOHMM(RU_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6bbda-fe27-418f-82e9-3556d04cee31",
   "metadata": {},
   "source": [
    "## 4.2 Calculation of Higher Order Transition Probabilities\n",
    "In addition to standard HMM initialization, consider the higher order transition probabilities. These probabilities involve multiple previous states, capturing longer-range dependencies.\n",
    "\n",
    "Simply put, we need to find the parameter $a_{u, v, q}$ which is the probability of state $q$ appearing after $u, v$ appears.\n",
    "$$a_{u, v, q} = P(q|y, v) = \\frac{Count(u, v, q)}{Count(u,v)}$$\n",
    "\n",
    "whereby $Count(u, v, q)$ is the number of times `q` appears after `u, v` appears before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e3131471-b2e0-45db-9065-8fbc9ac54014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_a_HOHMM(states, label_idx):\n",
    "    \"\"\"\n",
    "    Finds a_uvq, HO transition probabilities\n",
    "    INPUTS:\n",
    "    - states (arr): array containing states in dataset\n",
    "    - label_udx (arr): Array of Indexed Labels for each corresponding text\n",
    "    OUTPUTS:\n",
    "    - a_uvq (np.array): np.array containign transition probabilities\n",
    "    \"\"\"\n",
    "    # Find number of states\n",
    "    num_states = len(states)\n",
    "    \n",
    "    # Initiate a np array to store HO a_uvq\n",
    "    transition_probs = np.zeros((num_states, num_states, num_states))\n",
    "    # Finding Count(u, v, q)\n",
    "    for i in range(2, len(label_idx)):\n",
    "        prev_state_1 = label_idx[i - 2]\n",
    "        prev_state_2 = label_idx[i - 1]\n",
    "        current_state = label_idx[i]\n",
    "        \n",
    "        transition_probs[prev_state_1, prev_state_2, current_state] += 1\n",
    "        \n",
    "    # Finding HO transition params\n",
    "    count_uv = np.nansum(transition_probs, axis=2, keepdims=True) + 1e-10\n",
    "    a_uvq = transition_probs / count_uv\n",
    "    \n",
    "    # Handle NaNs, convert to 0 probability\n",
    "    a_uvq = np.nan_to_num(a_uvq, nan=0.0)\n",
    "\n",
    "    return a_uvq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "1ffd302a-6eb1-4818-9806-8ff8ace87549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding HO transition probabiltiies for ES and RU\n",
    "ES_a = find_a_HOHMM(ES_states, ES_label_idx)\n",
    "RU_a = find_a_HOHMM(RU_states, RU_label_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971673f-c19f-4d01-a443-511aeb8b83bd",
   "metadata": {},
   "source": [
    "## 4.3 Calculating Emission Probabilities\n",
    "Follows that of normal HMM\n",
    "$$b_u(o) = \\frac{Count(u \\to o)}{Count(u)}$$\n",
    "\n",
    "However, we modify the function to handle np arrays. In order to take into account possible unique words, we create a vocab mapping too to prevent IndexErrors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2dec43ea-7883-4b45-a39a-c607d9361751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_emission_probs(texts, labels, states):\n",
    "    \"\"\"\n",
    "    Calculates emission probabilities for a Higher Order Hidden Markov Model (HOHMM).\n",
    "\n",
    "    INPUTS:\n",
    "    - texts (list): List of observed words/tokens.\n",
    "    - labels (list): List of corresponding sentiment labels.\n",
    "    - states (list): List of all possible states (sentiment labels).\n",
    "\n",
    "    OUTPUTS:\n",
    "    - emission_probs (np.array): Emission probabilities matrix (state x word).\n",
    "    - words_to_idx (dict): Mappings from word to idx, following emissions probability np array\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    num_words = len(set(texts))\n",
    "    \n",
    "    # Emission probabilities (word given state)\n",
    "    emission_probs = np.zeros((num_states, num_words))\n",
    "    \n",
    "    # Count occurrences of words for each state\n",
    "    state_word_counts = np.zeros((num_states, num_words))\n",
    "\n",
    "    # Mapping dict\n",
    "    words_to_idx = {}\n",
    "    idx_to_words = {}\n",
    "    \n",
    "    # Process the dataset\n",
    "    for i in range(len(texts)):\n",
    "        state_idx = labels[i]  # Use label directly\n",
    "        word = texts[i]\n",
    "        word_idx = hash(word) % num_words  # Simplified hashing\n",
    "        # ensure no duplicates\n",
    "        while word_idx in idx_to_words and idx_to_words[word_idx] != word:\n",
    "            word_idx += 1\n",
    "        words_to_idx[word] = word_idx\n",
    "        idx_to_words[idx] = word\n",
    "        state_word_counts[state_idx, word_idx] += 1\n",
    "    \n",
    "    # Calculate emission probabilities\n",
    "    for state_idx in range(num_states):\n",
    "        total_count = np.sum(state_word_counts[state_idx])\n",
    "        emission_probs[state_idx] = state_word_counts[state_idx] / total_count\n",
    "    \n",
    "    return emission_probs, words_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "beebf78a-247f-4735-a5e4-44e20210d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RU_b, RU_wordmapping = calculate_emission_probs(RU_texts, RU_label_idx, RU_states)\n",
    "ES_b, ES_wordmapping = calculate_emission_probs(ES_texts, ES_label_idx, ES_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c00e1-1f98-44b6-9e3c-f99f644a2b2f",
   "metadata": {},
   "source": [
    "## 4.4 Start Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9d49eb47-d751-49e9-8ce5-e629d4169503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_start_probs(label_indices, num_states):\n",
    "    \"\"\"\n",
    "    Calculates start probabilities for a Higher Order Hidden Markov Model (HOHMM).\n",
    "\n",
    "    INPUTS:\n",
    "    - label_indices (list): List of label indices corresponding to each observation.\n",
    "    - num_states (int): Number of possible states (sentiment labels).\n",
    "\n",
    "    OUTPUTS:\n",
    "    - start_probs (np.array): Start probabilities vector for each state.\n",
    "    \"\"\"\n",
    "    start_counts = np.zeros(num_states)\n",
    "    \n",
    "    # Count occurrences of each state as the starting state\n",
    "    start_counts[label_indices[0]] += 1\n",
    "    \n",
    "    for i in range(1, len(label_indices)):\n",
    "        if label_indices[i - 1] != label_indices[i]:\n",
    "            start_counts[label_indices[i]] += 1\n",
    "    \n",
    "    # Calculate start probabilities\n",
    "    total_starts = np.sum(start_counts)\n",
    "    start_probs = start_counts / total_starts\n",
    "    \n",
    "    return start_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "64c26d44-28bf-419b-8289-eb6019ec9582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Probabilities for ES: [0.02094241 0.33653287 0.03926702 0.004363   0.11082024 0.01977894\n",
      " 0.46829552]\n",
      "Start Probabilities for RU: [0.0378497  0.33900165 0.06399707 0.00511977 0.08118486 0.0149936\n",
      " 0.45785336]\n"
     ]
    }
   ],
   "source": [
    "ES_start_probs = calculate_start_probs(ES_label_idx, len(ES_states))\n",
    "RU_start_probs = calculate_start_probs(RU_label_idx, len(RU_states))\n",
    "\n",
    "print(\"Start Probabilities for ES:\", ES_start_probs)\n",
    "print(\"Start Probabilities for RU:\", RU_start_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f9e89-3169-4f56-bd98-125f1302d082",
   "metadata": {},
   "source": [
    "## 4.5 Viterbi Algorithm for Best Sequences\n",
    "The function belows is differnt from the first due to the need of handling np.arrays, as well as a different way of calculating forward scores. Normally,\n",
    "$$\\pi(j+1, u) = \\max_v \\{\\pi(j, v) \\times b_u(x_{j+1}) \\times a_{v, u} \\}$$\n",
    "However, we need to take into account that we are not just using the score of the previous states, but that of 2 states ago as well. Hence,\n",
    "$$\\pi(j+1, q) = \\max_{u, v} \\{\\pi(j-1, u) \\times b_q(x_{j+1}) \\times a_{u, v} \\times a_{v, q} \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "048fe137-261c-485a-86f1-5c7a721d0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def viterbi_algorithm(observations, states, start_probs, transition_probs, emission_probs, word_mapping):\n",
    "    \"\"\"\n",
    "    Runs the Viterbi algorithm to find the most likely sequence of hidden states.\n",
    "\n",
    "    INPUTS:\n",
    "    - observations (list): List of observed words/tokens.\n",
    "    - states (list): List of all possible states (sentiment labels).\n",
    "    - start_probs (np.array): Initial state probabilities.\n",
    "    - transition_probs (np.array): Transition probabilities matrix (state x state x state).\n",
    "    - emission_probs (np.array): Emission probabilities matrix (state x word).\n",
    "    - word_mapping (dict): Mapping of words to index in emissions probabiltiy np array\n",
    "\n",
    "    OUTPUTS:\n",
    "    - best_path (list): List of the most likely sequence of hidden states.\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    num_observations = len(observations)\n",
    "    print(num_states, num_observations)\n",
    "    \n",
    "    # Create a Viterbi matrix to store probabilities and backpointers\n",
    "    viterbi_matrix = np.zeros((num_states, num_observations))\n",
    "    backpointers = np.zeros((num_states, num_observations), dtype=int)\n",
    "    \n",
    "    # Initialize the first column of the Viterbi matrix using start probabilities\n",
    "    viterbi_matrix[:, 0] = start_probs * emission_probs[:, word_mapping[observations[0]]]\n",
    "    \n",
    "    # Fill in the Viterbi matrix and backpointers\n",
    "    for t in range(1, num_observations):\n",
    "        if observations[t] in word_mapping:\n",
    "            for s in range(num_states):\n",
    "                max_prob = -np.inf\n",
    "                max_prev_state = None\n",
    "                for prev_state_1 in range(num_states):\n",
    "                    for prev_state_2 in range(num_states):\n",
    "                        prob = viterbi_matrix[prev_state_2, t - 1] * transition_probs[prev_state_1, prev_state_2, s] * emission_probs[s, word_mapping[observations[t]]]\n",
    "                        if prob > max_prob:\n",
    "                            max_prob = prob\n",
    "                            max_prev_state = prev_state_2\n",
    "                viterbi_matrix[s, t] = max_prob\n",
    "                backpointers[s, t] = max_prev_state\n",
    "        \n",
    "    # Backtrack to find the best path\n",
    "    best_path = [0] * num_observations\n",
    "    best_path[-1] = np.argmax(viterbi_matrix[:, -1])\n",
    "    for t in range(num_observations - 2, 1, -1):\n",
    "        best_path[t] = backpointers[best_path[t + 1], t+1]\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "# Example usage\n",
    "ES_states_indexed = [i for i in range(len(ES_states))]\n",
    "RU_states_indexed = [i for i in range(len(RU_states))]\n",
    "# best_path_ES = viterbi_algorithm(ES_texts, ES_states_indexed, ES_start_probs, ES_a, ES_b, ES_wordmapping)\n",
    "# best_path_RU = viterbi_algorithm(RU_texts, RU_states_indexed , RU_start_probs, RU_a, RU_b, RU_wordmapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4228247-946c-4a99-b2eb-36ff883c2fa3",
   "metadata": {},
   "source": [
    "## 4.6 Model Evaluation\n",
    "Now that we've found our parameters $a_{u, v, q}, b_q(o)$, we can run it on the development, before saving the files to find their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "43edb3c0-f96e-495f-a0e3-30804c29a485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 4312\n",
      "7 6589\n"
     ]
    }
   ],
   "source": [
    "# Data Processing for EU and RS\n",
    "ES_dev = \"ES/dev.in\"\n",
    "RU_dev = \"RU/dev.in\"\n",
    "\n",
    "dES_texts, dES_labels, dES_states, dES_state_mappings, dES_label_idx = process_data_HOHMM(ES_dev)\n",
    "dRU_texts, dRU_labels, dRU_states, dRU_state_mappings, dRU_label_idx = process_data_HOHMM(RU_dev)\n",
    "\n",
    "# Running viterbi using learned probabilities\n",
    "best_path_ES = viterbi_algorithm(dES_texts, ES_states_indexed, ES_start_probs, ES_a, ES_b, ES_wordmapping)\n",
    "best_path_RU = viterbi_algorithm(dRU_texts, RU_states_indexed , RU_start_probs, RU_a, RU_b, RU_wordmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "08f643e8-be48-4d25-a26c-433349b8660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-neutral': 0,\n",
       " 'B-positive': 1,\n",
       " 'I-positive': 2,\n",
       " 'I-neutral': 3,\n",
       " 'B-negative': 4,\n",
       " 'I-negative': 5,\n",
       " 'O': 6}"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RU_state_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ca0546df-22d5-444d-817c-d31c76443290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to BIO labels\n",
    "def convert_labels(best_path, state_mappings):\n",
    "    for i in range(len(best_path)):\n",
    "        best_path[i] = state_mappings[best_path[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "8ee55d59-8c57-42ca-9f7d-bb15b6b1acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping of idx to states \n",
    "r_map = {}\n",
    "for k, v in RU_state_mappings.items():\n",
    "    r_map[v] = k\n",
    "convert_labels(best_path_ES, r_map)\n",
    "convert_labels(best_path_RU, r_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbfb1b-a4f2-427d-ba9d-8ebbbe05d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the files\n",
    "write_sequences_to_file(\"ES/dev.p4.out\", ES_texts, best_path_ES)\n",
    "write_sequences_to_file(\"RU/dev.p4.out\", RU_texts, best_path_RU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cdcb3-1a45-47ae-982b-4d6552ba5596",
   "metadata": {},
   "source": [
    "### RU\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 434\n",
    "\n",
    "#Correct Entity : 68\n",
    "Entity  precision: 0.1567\n",
    "Entity  recall: 0.1748\n",
    "Entity  F: 0.1652\n",
    "\n",
    "#Correct Sentiment : 0\n",
    "Sentiment  precision: 0.0000\n",
    "Sentiment  recall: 0.0000\n",
    "Sentiment  F: 0.0000\n",
    "\n",
    "\n",
    "### ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 252\n",
    "\n",
    "#Correct Entity : 18\n",
    "Entity  precision: 0.0714\n",
    "Entity  recall: 0.0786\n",
    "Entity  F: 0.0748\n",
    "\n",
    "#Correct Sentiment : 0\n",
    "Sentiment  precision: 0.0000\n",
    "Sentiment  recall: 0.0000\n",
    "Sentiment  F: 0.0000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f083876-67a1-40a2-8b8c-2b581e41654a",
   "metadata": {},
   "source": [
    "Unfortunately, as seen from the test results, it may appear that a HOHMM might not be too optimal in our context as the training set might be too small to optimally calcu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
