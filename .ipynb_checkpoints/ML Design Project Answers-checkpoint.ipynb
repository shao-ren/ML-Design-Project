{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d838c1d",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7590be0",
   "metadata": {},
   "source": [
    "Write a function that estimates the emission parameters from the training set using MLE (maximum\n",
    "likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b373d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        file_content = file.read()\n",
    "    return file_content\n",
    "\n",
    "def write_file(file_path, content):\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5331aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_parameters(file_content):\n",
    "    tags = {}  # Dictionary to store counts of each observation\n",
    "    emission_params = {}  # Dictionary to store emission probabilities\n",
    "\n",
    "    # Split the file content into lines\n",
    "    lines = file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if line[-1] == 'O':\n",
    "            observation, tag = line[:-1], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-10], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-9], line[-9:]\n",
    "#         observation, tag = line.split()\n",
    "    \n",
    "        if tag in tags:\n",
    "            tags[tag][observation] = tags[tag].get(observation, 0) + 1\n",
    "        else:\n",
    "            tags[tag] = {observation: 1}\n",
    "\n",
    "    # Calculate emission probabilities for each observation and tag\n",
    "    for tag, observations_count in tags.items():\n",
    "        total_count = sum(observations_count.values())\n",
    "        emission_params[tag] = {\n",
    "            observation: count / total_count for observation, count in observations_count.items()\n",
    "        }\n",
    "\n",
    "    return emission_params\n",
    "\n",
    "file_content = read_file(\"ES/train.txt\")\n",
    "\n",
    "emission_parameters = estimate_emission_parameters(file_content)\n",
    "# print(emission_parameters, sum(len(words) for words in emission_parameters.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30067806",
   "metadata": {},
   "source": [
    "One problem with estimating the emission parameters is that some words that appear in the test set\n",
    "do not appear in the training set. One simple idea to handle this issue is as follows. We introduce\n",
    "a special word token #UNK#, and make the following modifications to the computation of emission\n",
    "probabilities.During the testing phase, if the word does not appear in the training set, we replace that word with\n",
    "#UNK#. Set k to 1, implement this fix into your function for computing the emission parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b13646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_parameters_modified(training_file_content, test_file_content, k=1):\n",
    "    tags = {}  # Dictionary to store counts of each observation\n",
    "    emission_params = {}  # Dictionary to store emission probabilities\n",
    "    train_words = []\n",
    "    test_words = []\n",
    "    \n",
    "    # Split the file content into lines\n",
    "    train_data_lines = training_file_content.strip().split(\"\\n\")\n",
    "    test_data_lines = test_file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line in train_data_lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if ' O' in line:\n",
    "            observation, tag = line[:-2], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-11], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-10], line[-9:]\n",
    "        if observation not in train_words:\n",
    "            train_words.append(observation)\n",
    "        if tag in tags:\n",
    "            tags[tag][observation] = tags[tag].get(observation, 0) + 1\n",
    "        else:\n",
    "            tags[tag] = {observation: 1}\n",
    "    # Iterate through each line to extract observations from test set\n",
    "    for line in test_data_lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if ' O' in line:\n",
    "            observation, tag = line[:-2], line[-1]\n",
    "        elif ('B-positive' in line \n",
    "            or 'I-positive' in line \n",
    "            or 'B-negative' in line \n",
    "            or 'I-negative' in line):\n",
    "            observation, tag = line[:-11], line[-10:]\n",
    "        elif ('B-neutral' in line\n",
    "            or 'I-neutral' in line):\n",
    "            observation, tag = line[:-10], line[-9:]\n",
    "        else:\n",
    "            observation = line\n",
    "        if observation not in test_words:\n",
    "            test_words.append(observation)\n",
    "    # Extract words that are in test set but not in train set   \n",
    "    unique_words = find_unique_words(test_words, train_words)\n",
    "\n",
    "    # Calculate emission probabilities for each observation and tag\n",
    "    for tag, observations_count in tags.items():\n",
    "        total_count = sum(observations_count.values())\n",
    "        emission_params[tag] = {\n",
    "            observation: count / (total_count + k) for observation, count in observations_count.items()\n",
    "        }\n",
    "        for word in unique_words:\n",
    "            emission_params[tag][word] = k / (total_count + k)\n",
    "\n",
    "\n",
    "    return emission_params, list(set(test_words).union(train_words))\n",
    "\n",
    "def find_unique_words(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    unique_in_list1 = set1 - set2\n",
    "\n",
    "    return list(unique_in_list1)\n",
    "\n",
    "\n",
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "emission_parameters = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "# print(emission_parameters, sum(len(words) for words in emission_parameters.values()))\n",
    "count = 0\n",
    "for emission_probabilities in emission_parameters.values():\n",
    "    count += len(emission_probabilities)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c12afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(path_to_train_file, path_to_test_file):\n",
    "    training_file_content = read_file(path_to_train_file)\n",
    "    test_file_content = read_file(path_to_test_file)\n",
    "    emission_parameters, words = estimate_emission_parameters_modified(training_file_content, test_file_content)\n",
    "    word_to_label = {}\n",
    "    for word in words:\n",
    "        probabilities = []\n",
    "        for label, freqs in emission_parameters.items():\n",
    "            if word in freqs:\n",
    "                probabilities.append((label, freqs[word]))\n",
    "        word_to_label[word] = max(probabilities, key=lambda x: x[1])[0]\n",
    "    return word_to_label\n",
    "\n",
    "def write_result_to_file(word_to_label, path_to_dev_set, path_to_output):\n",
    "    result = \"\"\n",
    "    dev_file_content = read_file(path_to_dev_set)\n",
    "    dev_set_lines = dev_file_content.strip().split(\"\\n\")\n",
    "    counter = 0\n",
    "    for line in dev_set_lines:\n",
    "        if line == \"\":\n",
    "            result += \"\\n\"\n",
    "            continue  \n",
    "        result += line + \" \" + word_to_label.get(line) + \"\\n\"\n",
    " \n",
    "    write_file(path_to_output, result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b9f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = sentiment_analysis(\"ES/train.txt\", \"ES/dev.in\")\n",
    "write_result_to_file(mapping, \"ES/dev.in\", \"ES/dev.p1.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03317db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = sentiment_analysis(\"RU/train.txt\", \"RU/dev.in\")\n",
    "write_result_to_file(mapping, \"RU/dev.in\", \"RU/dev.p1.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18d25a",
   "metadata": {},
   "source": [
    "## ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 1466\n",
    "\n",
    "#Correct Entity : 178\n",
    "Entity  precision: 0.1214\n",
    "Entity  recall: 0.7773\n",
    "Entity  F: 0.2100\n",
    "\n",
    "#Correct Sentiment : 97\n",
    "Sentiment  precision: 0.0662\n",
    "Sentiment  recall: 0.4236\n",
    "Sentiment  F: 0.1145\n",
    "\n",
    "## RU\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 1816\n",
    "\n",
    "#Correct Entity : 266\n",
    "Entity  precision: 0.1465\n",
    "Entity  recall: 0.6838\n",
    "Entity  F: 0.2413\n",
    "\n",
    "#Correct Sentiment : 129\n",
    "Sentiment  precision: 0.0710\n",
    "Sentiment  recall: 0.3316\n",
    "Sentiment  F: 0.1170"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64518f2",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cb2b5",
   "metadata": {},
   "source": [
    "Write a function that estimates the transition parameters from the training set using MLE (maximum\n",
    "likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62408ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'START': {'O': 0.9289176090468497, 'B-positive': 0.052234787291330104, 'B-negative': 0.014001077005923533, 'B-neutral': 0.004846526655896607}, 'O': {'O': 0.8856896848630963, 'B-positive': 0.03650766316514551, 'STOP': 0.06344067504735663, 'B-negative': 0.012226623041157224, 'B-neutral': 0.0021353538832443605}, 'B-positive': {'O': 0.871551724137931, 'I-positive': 0.11637931034482758, 'STOP': 0.008620689655172414, 'B-neutral': 0.0008620689655172414, 'B-positive': 0.002586206896551724}, 'B-negative': {'O': 0.8110236220472441, 'I-negative': 0.1784776902887139, 'STOP': 0.010498687664041995}, 'B-neutral': {'I-neutral': 0.20833333333333334, 'O': 0.7916666666666666}, 'I-neutral': {'I-neutral': 0.6511627906976745, 'O': 0.3488372093023256}, 'I-positive': {'I-positive': 0.5700636942675159, 'O': 0.4267515923566879, 'STOP': 0.0031847133757961785}, 'I-negative': {'O': 0.39766081871345027, 'I-negative': 0.6023391812865497}}\n"
     ]
    }
   ],
   "source": [
    "def estimate_transition_parameters(file_content):\n",
    "    transition_counts = {}\n",
    "    transition_params = {}  # Dictionary to store emission probabilities\n",
    "\n",
    "    # Split the file content into lines\n",
    "    lines = file_content.strip().split(\"\\n\")\n",
    "\n",
    "    # Iterate through each line and extract observations and tags\n",
    "    for line_index in range(len(lines)):\n",
    "        next_line = \"\"\n",
    "        curr_line = lines[line_index]\n",
    "        if line_index < len(lines) - 1:\n",
    "            next_line = lines[line_index + 1]\n",
    "        \n",
    "        if len(curr_line) == 0:\n",
    "            continue\n",
    "        if curr_line[-1] == 'O':\n",
    "            curr_observation, curr_tag = curr_line[:-1], curr_line[-1]\n",
    "        elif ('B-positive' in curr_line \n",
    "            or 'I-positive' in curr_line \n",
    "            or 'B-negative' in curr_line \n",
    "            or 'I-negative' in curr_line):\n",
    "            curr_observation, curr_tag = curr_line[:-10], curr_line[-10:]\n",
    "        elif ('B-neutral' in curr_line\n",
    "            or 'I-neutral' in curr_line):\n",
    "            curr_observation, curr_tag = curr_line[:-9], curr_line[-9:]\n",
    "        \n",
    "        if len(next_line) == 0: #true if line_index = len(lines - 1) or line_index is last index of a document\n",
    "            pass\n",
    "        elif next_line[-1] == 'O':\n",
    "            next_observation, next_tag = next_line[:-1], next_line[-1]\n",
    "        elif ('B-positive' in next_line \n",
    "            or 'I-positive' in next_line \n",
    "            or 'B-negative' in next_line \n",
    "            or 'I-negative' in next_line):\n",
    "            next_observation, next_tag = next_line[:-10], next_line[-10:]\n",
    "        elif ('B-neutral' in next_line\n",
    "            or 'I-neutral' in next_line):\n",
    "            next_observation, next_tag = next_line[:-9], next_line[-9:]        \n",
    "        \n",
    "        # Handle transitions from START\n",
    "        if line_index == 0 or lines[line_index - 1] == \"\":\n",
    "            if 'START' in transition_counts:\n",
    "                transition_counts['START'][curr_tag] = transition_counts['START'].get(curr_tag, 0) + 1\n",
    "            else:\n",
    "                transition_counts['START'] = {curr_tag: 1}\n",
    "                \n",
    "        # Handle transitions to STOP\n",
    "        if len(next_line) == 0:\n",
    "            if curr_tag in transition_counts:\n",
    "                transition_counts[curr_tag]['STOP'] = transition_counts[curr_tag].get('STOP', 0) + 1\n",
    "            else:\n",
    "                transition_counts[curr_tag] = {'STOP': 1}\n",
    "                \n",
    "        # Rest of the transitions\n",
    "        else:\n",
    "            if curr_tag in transition_counts:\n",
    "                transition_counts[curr_tag][next_tag] = transition_counts[curr_tag].get(next_tag, 0) + 1\n",
    "            else:\n",
    "                transition_counts[curr_tag] = {next_tag: 1}\n",
    "        \n",
    "\n",
    "    # Calculate transition probabilities \n",
    "    for tag, next_tag_counts in transition_counts.items():\n",
    "        total_count = sum(next_tag_counts.values())\n",
    "        transition_params[tag] = {\n",
    "            next_tag: count / total_count for next_tag, count in next_tag_counts.items()\n",
    "        }\n",
    "\n",
    "    return transition_params\n",
    "\n",
    "file_content = read_file(\"ES/train.txt\")\n",
    "\n",
    "transition_parameters = estimate_transition_parameters(file_content)\n",
    "print(transition_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181cb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(observation_sequence, emission_params, transition_params):\n",
    "    states = list(emission_params.keys())\n",
    "    T = len(observation_sequence)\n",
    "    N = len(states)\n",
    "\n",
    "    # Initialization step\n",
    "    viterbi_table = [{}]\n",
    "    backpointer = [{}]\n",
    "    for state in states:\n",
    "        viterbi_table[0][state] = transition_params['START'].get(state, 0) * emission_params[state].get(observation_sequence[0], 0)\n",
    "        backpointer[0][state] = None\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, T):\n",
    "        viterbi_table.append({})\n",
    "        backpointer.append({})\n",
    "        for state in states:\n",
    "            max_prob = max(\n",
    "                viterbi_table[t - 1][prev_state] * transition_params[prev_state].get(state, 0) * emission_params[state].get(observation_sequence[t], 0)\n",
    "                for prev_state in states\n",
    "            )\n",
    "            viterbi_table[t][state] = max_prob\n",
    "            backpointer[t][state] = max(states, key=lambda prev_state: viterbi_table[t - 1][prev_state] * transition_params[prev_state].get(state, 0))\n",
    "\n",
    "    # Termination step\n",
    "    max_prob_last_state = max(viterbi_table[T - 1].values())\n",
    "    best_last_state = max(states, key=lambda state: viterbi_table[T - 1][state] * transition_params[state].get('STOP', 0))\n",
    "\n",
    "    # Backtrack to find the best sequence\n",
    "    best_sequence = [best_last_state]\n",
    "    prev_state = best_last_state\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        best_sequence.insert(0, backpointer[t + 1][prev_state])\n",
    "        prev_state = backpointer[t + 1][prev_state]\n",
    "\n",
    "    return best_sequence\n",
    "\n",
    "def create_observation_sequences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        file_content = file.read().strip()\n",
    "\n",
    "    observation_sequences = []\n",
    "    current_sequence = []\n",
    "\n",
    "    # Split the file content by empty lines to get individual sequences\n",
    "    sequence_lines = file_content.split(\"\\n\\n\")\n",
    "\n",
    "    for sequence_line in sequence_lines:\n",
    "        # Split each sequence into individual observations (words)\n",
    "        observation_sequence = sequence_line.strip().split(\"\\n\")\n",
    "        current_sequence.extend(observation_sequence)\n",
    "        observation_sequences.append(current_sequence)\n",
    "        current_sequence = []\n",
    "\n",
    "    return observation_sequences\n",
    "\n",
    "def viterbi_for_sequences(observation_sequences, emission_params, transition_params):\n",
    "    best_sequences = []\n",
    "    for observation_sequence in observation_sequences:\n",
    "        best_sequence = viterbi(observation_sequence, emission_params, transition_params)\n",
    "        best_sequences.append(best_sequence)\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "def write_sequences_to_file(output_file_path, observation_sequences, best_sequences):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for obs_sequence, best_sequence in zip(observation_sequences, best_sequences):\n",
    "            for word, tag in zip(obs_sequence, best_sequence):\n",
    "                file.write(f\"{word} {tag}\\n\")\n",
    "            file.write(\"\\n\")  # Separate sequences with an empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a315bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_content = read_file(\"ES/train.txt\")\n",
    "test_file_content = read_file(\"ES/dev.in\")\n",
    "emission_params = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "transition_params = estimate_transition_parameters(training_file_content)\n",
    "observation_sequences = create_observation_sequences(\"ES/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"ES/dev.p2.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dac0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_content = read_file(\"RU/train.txt\")\n",
    "test_file_content = read_file(\"RU/dev.in\")\n",
    "emission_params = estimate_emission_parameters_modified(training_file_content, test_file_content, 1)[0]\n",
    "transition_params = estimate_transition_parameters(training_file_content)\n",
    "observation_sequences = create_observation_sequences(\"RU/dev.in\")\n",
    "best_sequences = viterbi_for_sequences(observation_sequences, emission_params, transition_params)\n",
    "# print(best_sequences)\n",
    "   \n",
    "write_sequences_to_file(\"RU/dev.p2.out\", observation_sequences, best_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56d1b4",
   "metadata": {},
   "source": [
    "## ES\n",
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 542\n",
    "\n",
    "#Correct Entity : 134\n",
    "Entity  precision: 0.2472\n",
    "Entity  recall: 0.5852\n",
    "Entity  F: 0.3476\n",
    "\n",
    "#Correct Sentiment : 97\n",
    "Sentiment  precision: 0.1790\n",
    "Sentiment  recall: 0.4236\n",
    "Sentiment  F: 0.2516"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d21ca7",
   "metadata": {},
   "source": [
    "## RU\n",
    "\n",
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 514\n",
    "\n",
    "#Correct Entity : 190\n",
    "Entity  precision: 0.3696\n",
    "Entity  recall: 0.4884\n",
    "Entity  F: 0.4208\n",
    "\n",
    "#Correct Sentiment : 129\n",
    "Sentiment  precision: 0.2510\n",
    "Sentiment  recall: 0.3316\n",
    "Sentiment  F: 0.2857"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875a221-d46d-460a-a531-c516476069e6",
   "metadata": {},
   "source": [
    "# Part 4: Design Challenge\n",
    "For our design, we propose the use of Higher Order Hidden Markov Models (HOHMM). \n",
    "In a Higher Order HMM, emission pand transition probabilities are conditioned on the current state and the previous several states. This allows the model to capture longer-range dependencies in the sequence. For sentiment analysis, this could help capture more complex sentiment patterns that extend over multiple words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab635ea-c0d8-4528-9f80-26fa06bf33ac",
   "metadata": {},
   "source": [
    "## 4.1 Data Processing\n",
    "Here, we aim to use integers to index each possible state in order. This is done so as to enable the use of numpy arrays for calculating of Higher-order Transition Probabilities, that take into account not just the previous state but one before it that will be done in section `4.2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3583ba2e-fe32-43a6-bdf2-017f63c2345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5382e56-e337-46ec-a674-e304de702546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for this step 4.1\n",
    "def read_data_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Seperates txt file into texts and labels arary\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                text = parts[0]\n",
    "                label = parts[1]\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "def find_HO_transition_probabilities(file_path):\n",
    "    \"\"\"\n",
    "    Finds higher order transition probabilities\n",
    "    INPUTS:\n",
    "    - file_path (string): Path to the data file in txt format\n",
    "    OUTPUTS:\n",
    "    - texts: array containing all texts\n",
    "    - labels: array containing coressponding labels of each text\n",
    "    - states: array containing unique states\n",
    "    - state_to_idx: mapping of states to a numerical index\n",
    "    - label_idx: array of statesin numerical idx, corresponding to texts\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    texts, labels = read_data_from_file(file_path)\n",
    "    # Get states\n",
    "    states = list(set(labels))\n",
    "    num_states = len(states)\n",
    "    \n",
    "    \n",
    "    # Define a mapping from states to indices\n",
    "    state_to_idx = {state: idx for idx, state in enumerate(states)}\n",
    "    \n",
    "    # Get array of state indices corresponding to each text in the texts array\n",
    "    label_idx = [state_to_idx[label] for label in labels]\n",
    "\n",
    "    return texts, labels, states, state_to_idx, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acb8b9f0-e3c2-4d6f-970c-aa80c1462ad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m ES_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mES/train.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m RU_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRU/train.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m ES_texts, ES_labels, ES_states, ES_state_mappings, ES_label_idx \u001b[38;5;241m=\u001b[39m process_data_HOHMM(ES_train)\n\u001b[1;32m      5\u001b[0m RU_texts, RU_labels, RU_states, RU_state_mappings, RU_label_idx \u001b[38;5;241m=\u001b[39m process_data_HOHMM(RU_train)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "ES_train = \"ES/train.txt\"\n",
    "RU_train = \"RU/train.txt\"\n",
    "\n",
    "ES_texts, ES_labels, ES_states, ES_state_mappings, ES_label_idx = process_data_HOHMM(ES_train)\n",
    "RU_texts, RU_labels, RU_states, RU_state_mappings, RU_label_idx = process_data_HOHMM(RU_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6bbda-fe27-418f-82e9-3556d04cee31",
   "metadata": {},
   "source": [
    "## 4.2 Calculation of Higher Order Transition Probabilities\n",
    "In addition to standard HMM initialization, consider the higher order transition probabilities. These probabilities involve multiple previous states, capturing longer-range dependencies.\n",
    "\n",
    "Simply put, we need to find the parameter $a_{u, v, q}$ which is the probability of state $q$ appearing after $u, v$ appears.\n",
    "$$a_{u, v, q} = P(q|y, v) = \\frac{Count(u, v, q)}{Count(u,v)}$$\n",
    "\n",
    "whereby $Count(u, v, q)$ is the number of times `q` appears after `u, v` appears before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3131471-b2e0-45db-9065-8fbc9ac54014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_a_HOHMM(states):\n",
    "    \"\"\"\n",
    "    Finds a_uvq, HO transition probabilities\n",
    "    INPUTS:\n",
    "    - states (arr): array containing states in dataset\n",
    "    OUTPUTS:\n",
    "    - a_uvq (np.array): np.array containign transition probabilities\n",
    "    \"\"\"\n",
    "    # Find number of states\n",
    "    num_states = len(states)\n",
    "    \n",
    "    # Initiate a np array to store HO a_uvq\n",
    "    transition_probs = np.zeros((num_states, num_states, num_states))\n",
    "    # Finding Count(u, v, q)\n",
    "    for i in range(2, len(label_idx)):\n",
    "        prev_state_1 = label_idx[i - 2]\n",
    "        prev_state_2 = label_idx[i - 1]\n",
    "        current_state = label_idx[i]\n",
    "        \n",
    "        transition_probs[prev_state_1, prev_state_2, current_state] += 1\n",
    "        \n",
    "    # Finding HO transition params\n",
    "    count_uv = np.nansum(transition_probs, axis=2, keepdims=True) \n",
    "    a_uvq = transition_probs / count_uv\n",
    "    \n",
    "    # Handle NaNs, convert to 0 probability\n",
    "    a_uvq = np.nan_to_num(a_uvq, nan=0.0)\n",
    "\n",
    "    return a_uvq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ffd302a-6eb1-4818-9806-8ff8ace87549",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ES_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ES_a \u001b[38;5;241m=\u001b[39m find_a_HOHMM(\u001b[43mES_states\u001b[49m)\n\u001b[1;32m      2\u001b[0m RU_a \u001b[38;5;241m=\u001b[39m find_a_HOHMM(RU_states)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ES_states' is not defined"
     ]
    }
   ],
   "source": [
    "ES_a = find_a_HOHMM(ES_states)\n",
    "RU_a = find_a_HOHMM(RU_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971673f-c19f-4d01-a443-511aeb8b83bd",
   "metadata": {},
   "source": [
    "## 4.3 Calculating Emission Probabilities\n",
    "Follows that of normal HMM\n",
    "$$b_u(o) = \\frac{Count(u \\to o)}{Count(u)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09b32f-c370-4f60-90b8-f489321b091b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
